#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥å¿—è§£æå™¨ - è§£ææ•°æ®åº“å’ŒWebæœåŠ¡æ—¥å¿—ï¼Œæå–å…³é”®å­—
"""

import re
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import os
from collections import defaultdict, Counter


class LogParser:
    """æ—¥å¿—è§£æå™¨"""
    
    def __init__(self, output_dir: str = "data"):
        """åˆå§‹åŒ–æ—¥å¿—è§£æå™¨"""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # è¾“å‡ºæ–‡ä»¶
        self.parsed_logs_file = self.output_dir / "parsed_logs.json"
        self.log_summary_file = self.output_dir / "log_summary.json"
        
        # å¸¸è§æ—¥å¿—è·¯å¾„
        self.common_log_paths = {
            'nginx': [
                '/var/log/nginx/error.log',
                '/var/log/nginx/access.log',
                'C:/nginx/logs/error.log',
                'C:/nginx/logs/access.log'
            ],
            'mysql': [
                '/var/log/mysql/error.log',
                '/var/log/mysqld.log',
                '/var/log/mysqld_exporter_metrics.log',
                'C:/ProgramData/MySQL/MySQL Server 8.0/Data/*.err'
            ],
            'apache': [
                '/var/log/apache2/error.log',
                '/var/log/httpd/error_log',
                'C:/Apache24/logs/error.log'
            ],
            'redis': [
                '/var/log/redis/redis-server.log',
                'C:/Redis/logs/redis.log'
            ],
            'postgresql': [
                '/var/log/postgresql/postgresql.log',
                'C:/Program Files/PostgreSQL/*/data/log/*.log'
            ],
            'blackbox': [
                '/var/log/blackbox_exporter_metrics.log'
            ],
            'loki': [
                '/var/log/loki.log'
            ],
            'system': [
                '/var/log/messages-*'
            ],
            'node_exporter': [
                '/var/log/node_exporter_metrics.log'
            ],
            'promptail': [
                '/var/log/promtail.log'
            ],
            
        }
        
        # å…³é”®è¯æ¨¡å¼
        self.error_patterns = {
            'critical': [
                r'critical|fatal|emergency|panic',
                r'out of memory|memory leak|oom',
                r'segmentation fault|core dump',
                r'database.*corrupt|table.*corrupt',
                r'connection.*refused|connection.*failed'
            ],
            'error': [
                r'error|err|failed|failure',
                r'timeout|time.*out',
                r'502|503|504|500',
                r'upstream.*failed|backend.*failed',
                r'access denied|permission denied',
                r'cannot connect|connection lost'
            ],
            'warning': [
                r'warning|warn',
                r'slow query|slow.*log',
                r'high load|high.*usage',
                r'retry|retrying',
                r'deprecated|obsolete'
            ]
        }
        
        # æœåŠ¡ç‰¹å®šæ¨¡å¼
        self.service_patterns = {
            'nginx': {
                'error_codes': r'(\d{3})\s+\d+',
                'upstream': r'upstream.*?(timeout|failed|error)',
                'client_errors': r'client.*?(closed|reset|timeout)',
                'response_time': r'request_time:(\d+\.\d+)'
            },
            'mysql': {
                'innodb_errors': r'InnoDB.*?(error|warning|corruption)',
                'connection_errors': r'connection.*?(failed|refused|timeout)',
                'query_errors': r'query.*?(error|failed|timeout)',
                'table_errors': r'table.*?(corrupt|crashed|error)'
            },
            'apache': {
                'mod_errors': r'mod_\w+.*?(error|failed)',
                'client_errors': r'client.*?(timeout|reset|denied)',
                'server_errors': r'server.*?(error|failed|timeout)'
            }
        }
        
        # æ—¶é—´æ ¼å¼
        self.time_patterns = [
            r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})',  # MySQL: 2024-01-01 12:00:00
            r'(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})',   # Nginx: 01/Jan/2024:12:00:00
            r'(\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2})',   # 2024/01/01 12:00:00
            r'(\w{3} \d{2} \d{2}:\d{2}:\d{2})'          # Jan 01 12:00:00
        ]
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def find_log_files(self, service: str = None) -> Dict[str, List[str]]:
        """æŸ¥æ‰¾ç³»ç»Ÿä¸­çš„æ—¥å¿—æ–‡ä»¶"""
        found_logs = defaultdict(list)
        
        # å¦‚æœæŒ‡å®šäº†æœåŠ¡ï¼ŒåªæŸ¥æ‰¾è¯¥æœåŠ¡çš„æ—¥å¿—
        if service and service in self.common_log_paths:
            service_paths = {service: self.common_log_paths[service]}
        else:
            service_paths = self.common_log_paths
        
        for svc, paths in service_paths.items():
            for path_pattern in paths:
                try:
                    # å¤„ç†é€šé…ç¬¦è·¯å¾„
                    if '*' in path_pattern:
                        import glob
                        for path in glob.glob(path_pattern):
                            if os.path.exists(path) and os.path.isfile(path):
                                found_logs[svc].append(path)
                    else:
                        if os.path.exists(path_pattern) and os.path.isfile(path_pattern):
                            found_logs[svc].append(path_pattern)
                except Exception as e:
                    self.logger.debug(f"æŸ¥æ‰¾æ—¥å¿—è·¯å¾„å¤±è´¥ {path_pattern}: {e}")
        
        return dict(found_logs)
    
    def extract_timestamp(self, line: str) -> Optional[datetime]:
        """ä»æ—¥å¿—è¡Œä¸­æå–æ—¶é—´æˆ³"""
        for pattern in self.time_patterns:
            match = re.search(pattern, line)
            if match:
                time_str = match.group(1)
                
                # å°è¯•è§£æä¸åŒæ ¼å¼çš„æ—¶é—´
                time_formats = [
                    '%Y-%m-%d %H:%M:%S',
                    '%d/%b/%Y:%H:%M:%S',
                    '%Y/%m/%d %H:%M:%S',
                    '%b %d %H:%M:%S'
                ]
                
                for fmt in time_formats:
                    try:
                        if fmt == '%b %d %H:%M:%S':
                            # ä¸ºæ²¡æœ‰å¹´ä»½çš„æ ¼å¼æ·»åŠ å½“å‰å¹´ä»½
                            time_str = f"{datetime.now().year} {time_str}"
                            fmt = '%Y %b %d %H:%M:%S'
                        return datetime.strptime(time_str, fmt)
                    except ValueError:
                        continue
        
        return None
    
    def classify_log_level(self, line: str) -> str:
        """åˆ†ç±»æ—¥å¿—çº§åˆ«"""
        line_lower = line.lower()
        
        # æ£€æŸ¥criticalçº§åˆ«
        for pattern in self.error_patterns['critical']:
            if re.search(pattern, line_lower):
                return 'critical'
        
        # æ£€æŸ¥errorçº§åˆ«
        for pattern in self.error_patterns['error']:
            if re.search(pattern, line_lower):
                return 'error'
        
        # æ£€æŸ¥warningçº§åˆ«
        for pattern in self.error_patterns['warning']:
            if re.search(pattern, line_lower):
                return 'warning'
        
        return 'info'
    
    def extract_service_metrics(self, line: str, service: str) -> Dict[str, Any]:
        """æå–æœåŠ¡ç‰¹å®šçš„æŒ‡æ ‡"""
        metrics = {}
        
        if service in self.service_patterns:
            patterns = self.service_patterns[service]
            
            for metric_name, pattern in patterns.items():
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    if metric_name == 'error_codes':
                        metrics['http_status'] = int(match.group(1))
                    elif metric_name == 'response_time':
                        metrics['response_time'] = float(match.group(1))
                    else:
                        metrics[metric_name] = match.group(0)
        
        return metrics
    
    def parse_log_file(self, file_path: str, service: str, 
                      lines_limit: int = 1000, 
                      time_window_hours: int = 24) -> Dict[str, Any]:
        """è§£æå•ä¸ªæ—¥å¿—æ–‡ä»¶"""
        parsed_data = {
            'file_path': file_path,
            'service': service,
            'parse_time': datetime.now().isoformat(),
            'total_lines': 0,
            'parsed_lines': 0,
            'log_entries': [],
            'summary': {
                'by_level': Counter(),
                'by_hour': defaultdict(int),
                'error_keywords': Counter(),
                'service_metrics': defaultdict(list)
            }
        }
        
        # è®¡ç®—æ—¶é—´çª—å£
        cutoff_time = datetime.now() - timedelta(hours=time_window_hours)
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
                parsed_data['total_lines'] = len(lines)
                
                # ä»æ–‡ä»¶æœ«å°¾å¼€å§‹è¯»å–ï¼Œè·å–æœ€æ–°çš„æ—¥å¿—
                for line in reversed(lines[-lines_limit:]):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # æå–æ—¶é—´æˆ³
                    timestamp = self.extract_timestamp(line)
                    if timestamp and timestamp < cutoff_time:
                        continue  # è·³è¿‡è¶…å‡ºæ—¶é—´çª—å£çš„æ—¥å¿—
                    
                    # åˆ†ç±»æ—¥å¿—çº§åˆ«
                    log_level = self.classify_log_level(line)
                    
                    # æå–æœåŠ¡æŒ‡æ ‡
                    service_metrics = self.extract_service_metrics(line, service)
                    
                    # æ„å»ºæ—¥å¿—æ¡ç›®
                    log_entry = {
                        'timestamp': timestamp.isoformat() if timestamp else None,
                        'level': log_level,
                        'message': line,
                        'service_metrics': service_metrics
                    }
                    
                    parsed_data['log_entries'].append(log_entry)
                    parsed_data['parsed_lines'] += 1
                    
                    # æ›´æ–°ç»Ÿè®¡
                    parsed_data['summary']['by_level'][log_level] += 1
                    
                    if timestamp:
                        hour_key = timestamp.strftime('%Y-%m-%d %H:00')
                        parsed_data['summary']['by_hour'][hour_key] += 1
                    
                    # æå–é”™è¯¯å…³é”®è¯
                    if log_level in ['error', 'critical']:
                        words = line.lower().split()
                        for word in words:
                            if len(word) > 3:  # è¿‡æ»¤çŸ­è¯
                                parsed_data['summary']['error_keywords'][word] += 1
                    
                    # è®°å½•æœåŠ¡æŒ‡æ ‡
                    for metric, value in service_metrics.items():
                        parsed_data['summary']['service_metrics'][metric].append(value)
                
                # å€’åºå›æ­£åº
                parsed_data['log_entries'].reverse()
                
        except Exception as e:
            self.logger.error(f"è§£ææ—¥å¿—æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            parsed_data['error'] = str(e)
        
        return parsed_data
    
    def parse_all_logs(self, time_window_hours: int = 24, 
                      lines_per_file: int = 1000) -> Dict[str, Any]:
        """è§£ææ‰€æœ‰æ‰¾åˆ°çš„æ—¥å¿—æ–‡ä»¶"""
        self.logger.info("å¼€å§‹è§£ææ—¥å¿—æ–‡ä»¶...")
        
        # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
        found_logs = self.find_log_files()
        
        all_parsed_data = {
            'parse_time': datetime.now().isoformat(),
            'time_window_hours': time_window_hours,
            'services': {},
            'global_summary': {
                'total_files': 0,
                'total_lines': 0,
                'total_errors': 0,
                'total_warnings': 0,
                'total_criticals': 0,
                'top_error_keywords': [],
                'services_found': []
            }
        }
        
        if not found_logs:
            self.logger.warning("æœªæ‰¾åˆ°ä»»ä½•æ—¥å¿—æ–‡ä»¶")
            return all_parsed_data
        
        # è§£ææ¯ä¸ªæœåŠ¡çš„æ—¥å¿—
        for service, log_files in found_logs.items():
            self.logger.info(f"è§£æ {service} æœåŠ¡æ—¥å¿—ï¼Œå…± {len(log_files)} ä¸ªæ–‡ä»¶")
            
            service_data = {
                'service': service,
                'files': [],
                'summary': {
                    'total_lines': 0,
                    'by_level': Counter(),
                    'recent_errors': [],
                    'service_metrics': {}
                }
            }
            
            for log_file in log_files:
                file_data = self.parse_log_file(
                    log_file, service, lines_per_file, time_window_hours
                )
                service_data['files'].append(file_data)
                
                # æ›´æ–°æœåŠ¡ç»Ÿè®¡
                service_data['summary']['total_lines'] += file_data['parsed_lines']
                
                for level, count in file_data['summary']['by_level'].items():
                    service_data['summary']['by_level'][level] += count
                
                # æ”¶é›†æœ€è¿‘çš„é”™è¯¯
                for entry in file_data['log_entries']:
                    if entry['level'] in ['error', 'critical']:
                        service_data['summary']['recent_errors'].append({
                            'timestamp': entry['timestamp'],
                            'level': entry['level'],
                            'message': entry['message'][:200] + '...' if len(entry['message']) > 200 else entry['message']
                        })
                
                # åˆå¹¶æœåŠ¡æŒ‡æ ‡
                for metric, values in file_data['summary']['service_metrics'].items():
                    if metric not in service_data['summary']['service_metrics']:
                        service_data['summary']['service_metrics'][metric] = []
                    service_data['summary']['service_metrics'][metric].extend(values)
            
            # é™åˆ¶recent_errorsæ•°é‡
            service_data['summary']['recent_errors'] = \
                service_data['summary']['recent_errors'][-50:]
            
            all_parsed_data['services'][service] = service_data
            all_parsed_data['global_summary']['services_found'].append(service)
        
        # è®¡ç®—å…¨å±€ç»Ÿè®¡
        all_error_keywords = Counter()
        for service_data in all_parsed_data['services'].values():
            all_parsed_data['global_summary']['total_files'] += len(service_data['files'])
            all_parsed_data['global_summary']['total_lines'] += service_data['summary']['total_lines']
            
            for level, count in service_data['summary']['by_level'].items():
                if level == 'error':
                    all_parsed_data['global_summary']['total_errors'] += count
                elif level == 'warning':
                    all_parsed_data['global_summary']['total_warnings'] += count
                elif level == 'critical':
                    all_parsed_data['global_summary']['total_criticals'] += count
            
            # æ”¶é›†é”™è¯¯å…³é”®è¯
            for file_data in service_data['files']:
                all_error_keywords.update(file_data['summary']['error_keywords'])
        
        # æå–é¡¶çº§é”™è¯¯å…³é”®è¯
        all_parsed_data['global_summary']['top_error_keywords'] = \
            [{'keyword': word, 'count': count} 
             for word, count in all_error_keywords.most_common(20)]
        
        return all_parsed_data
    
    def save_parsed_logs(self, parsed_data: Dict[str, Any]):
        """ä¿å­˜è§£æç»“æœ"""
        try:
            # ä¿å­˜è¯¦ç»†æ•°æ®
            with open(self.parsed_logs_file, 'w', encoding='utf-8') as f:
                json.dump(parsed_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æ‘˜è¦æ•°æ®
            summary = {
                'parse_time': parsed_data['parse_time'],
                'global_summary': parsed_data['global_summary'],
                'services_summary': {}
            }
            
            for service, service_data in parsed_data['services'].items():
                summary['services_summary'][service] = {
                    'total_lines': service_data['summary']['total_lines'],
                    'by_level': dict(service_data['summary']['by_level']),
                    'files_count': len(service_data['files']),
                    'recent_errors_count': len(service_data['summary']['recent_errors'])
                }
            
            with open(self.log_summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"æ—¥å¿—è§£æç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜è§£æç»“æœå¤±è´¥: {e}")
    
    def get_error_patterns(self, service: str = None) -> List[Dict[str, Any]]:
        """è·å–é”™è¯¯æ¨¡å¼"""
        try:
            with open(self.parsed_logs_file, 'r', encoding='utf-8') as f:
                parsed_data = json.load(f)
            
            patterns = []
            
            # å¦‚æœæŒ‡å®šæœåŠ¡ï¼Œåªè¿”å›è¯¥æœåŠ¡çš„æ¨¡å¼
            if service and service in parsed_data['services']:
                services_to_check = {service: parsed_data['services'][service]}
            else:
                services_to_check = parsed_data['services']
            
            for svc, svc_data in services_to_check.items():
                for file_data in svc_data['files']:
                    # åˆ†æé”™è¯¯å’Œcriticalçº§åˆ«çš„æ—¥å¿—
                    error_entries = [
                        entry for entry in file_data['log_entries']
                        if entry['level'] in ['error', 'critical']
                    ]
                    
                    if error_entries:
                        # æŒ‰å°æ—¶åˆ†ç»„é”™è¯¯
                        hourly_errors = defaultdict(list)
                        for entry in error_entries:
                            if entry['timestamp']:
                                hour = entry['timestamp'][:13] + ':00:00'  # å–åˆ°å°æ—¶
                                hourly_errors[hour].append(entry)
                        
                        # æ‰¾å‡ºé”™è¯¯é¢‘ç¹çš„æ—¶æ®µ
                        for hour, entries in hourly_errors.items():
                            if len(entries) >= 3:  # ä¸€å°æ—¶å†…3ä¸ªæˆ–ä»¥ä¸Šé”™è¯¯
                                pattern = {
                                    'service': svc,
                                    'time_period': hour,
                                    'error_count': len(entries),
                                    'severity': 'high' if any(e['level'] == 'critical' for e in entries) else 'medium',
                                    'sample_messages': [e['message'][:100] for e in entries[:3]],
                                    'common_keywords': self._extract_common_keywords([e['message'] for e in entries])
                                }
                                patterns.append(pattern)
            
            return patterns
            
        except Exception as e:
            self.logger.error(f"è·å–é”™è¯¯æ¨¡å¼å¤±è´¥: {e}")
            return []
    
    def _extract_common_keywords(self, messages: List[str]) -> List[str]:
        """ä»æ¶ˆæ¯åˆ—è¡¨ä¸­æå–å¸¸è§å…³é”®è¯"""
        all_words = []
        for msg in messages:
            words = re.findall(r'\b\w{4,}\b', msg.lower())  # æå–4å­—ç¬¦ä»¥ä¸Šçš„å•è¯
            all_words.extend(words)
        
        word_counts = Counter(all_words)
        # è¿”å›å‡ºç°2æ¬¡ä»¥ä¸Šçš„è¯
        return [word for word, count in word_counts.items() if count >= 2]


def main():
    """ä¸»å‡½æ•°æ¼”ç¤º"""
    parser = LogParser(output_dir="data")
    
    print("ğŸ” å¼€å§‹æ—¥å¿—è§£æ...")
    
    try:
        # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
        found_logs = parser.find_log_files()
        print(f"ğŸ“ å‘ç°æ—¥å¿—æ–‡ä»¶:")
        for service, files in found_logs.items():
            print(f"  - {service}: {len(files)} ä¸ªæ–‡ä»¶")
            for file_path in files[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"    â€¢ {file_path}")
        
        if not found_logs:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ—¥å¿—æ–‡ä»¶")
            return
        
        # è§£ææ‰€æœ‰æ—¥å¿—
        parsed_data = parser.parse_all_logs(time_window_hours=24)
        
        # ä¿å­˜ç»“æœ
        parser.save_parsed_logs(parsed_data)
        
        # æ˜¾ç¤ºæ‘˜è¦
        summary = parsed_data['global_summary']
        print(f"\nğŸ“Š è§£ææ‘˜è¦:")
        print(f"  - å¤„ç†æ–‡ä»¶æ•°: {summary['total_files']}")
        print(f"  - è§£æè¡Œæ•°: {summary['total_lines']}")
        print(f"  - é”™è¯¯æ•°: {summary['total_errors']}")
        print(f"  - è­¦å‘Šæ•°: {summary['total_warnings']}")
        print(f"  - ä¸¥é‡é”™è¯¯æ•°: {summary['total_criticals']}")
        
        if summary['top_error_keywords']:
            print(f"\nğŸ”‘ å¸¸è§é”™è¯¯å…³é”®è¯:")
            for kw in summary['top_error_keywords'][:5]:
                print(f"  - {kw['keyword']}: {kw['count']} æ¬¡")
        
        # è·å–é”™è¯¯æ¨¡å¼
        patterns = parser.get_error_patterns()
        if patterns:
            print(f"\nâš ï¸ å‘ç° {len(patterns)} ä¸ªé”™è¯¯æ¨¡å¼:")
            for pattern in patterns[:3]:
                print(f"  - {pattern['service']} ({pattern['time_period']}): "
                      f"{pattern['error_count']} ä¸ªé”™è¯¯")
        
        print(f"\nğŸ’¾ è§£æç»“æœå·²ä¿å­˜åˆ°: {parser.output_dir}")
        
    except Exception as e:
        print(f"âŒ æ—¥å¿—è§£æå¤±è´¥: {e}")


if __name__ == "__main__":
    main() 