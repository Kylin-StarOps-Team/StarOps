#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼‚å¸¸æ¨¡å¼æ•æ‰å’Œæ‰«ææµ‹è¯•çš„ç»¼åˆæµ‹è¯•è„šæœ¬
ç”¨äºè®ºæ–‡æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
"""

import time
import json
import logging
import psutil
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Tuple
import subprocess
import sys
from prettytable import PrettyTable
import matplotlib.pyplot as plt
import numpy as np

# å¯¼å…¥ä¸»è¦æ¨¡å—
from main import AnomalyDetectionSystem
from collect_metrics import MetricsCollector
from parse_logs import LogParser
from detect_anomaly import AnomalyDetector
from extract_pattern import PatternExtractor
from generate_scanner import ScannerGenerator

class ComprehensiveTestSuite:
    """ç»¼åˆæµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, test_output_dir: str = "test_results"):
        """åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶"""
        self.test_output_dir = Path(test_output_dir)
        self.test_output_dir.mkdir(exist_ok=True)
        
        # æµ‹è¯•ç»“æœå­˜å‚¨
        self.test_results = {
            'test_start_time': datetime.now().isoformat(),
            'system_info': self._get_system_info(),
            'tests': {}
        }
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ
        self.ads = AnomalyDetectionSystem(
            data_dir="data",
            scanners_dir="scanners"
        )
    
    def _setup_logging(self):
        """è®¾ç½®æµ‹è¯•æ—¥å¿—"""
        log_file = self.test_output_dir / "test_execution.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
    
    def _get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        import platform
        return {
            'cpu_count': psutil.cpu_count(),
            'memory_total_gb': round(psutil.virtual_memory().total / (1024**3), 2),
            'disk_total_gb': round(psutil.disk_usage('/').total / (1024**3), 2),
            'python_version': sys.version.split()[0],
            'platform': {
                'system': platform.system(),
                'node': platform.node(),
                'release': platform.release(),
                'machine': platform.machine()
            }
        }
    
    def test_1_data_collection_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•1: æ•°æ®æ”¶é›†æ€§èƒ½æµ‹è¯•"""
        self.logger.info("=" * 60)
        self.logger.info("æµ‹è¯•1: æ•°æ®æ”¶é›†æ€§èƒ½æµ‹è¯•")
        self.logger.info("=" * 60)
        
        test_result = {
            'test_name': 'æ•°æ®æ”¶é›†æ€§èƒ½æµ‹è¯•',
            'start_time': datetime.now().isoformat(),
            'success': False,
            'metrics': {}
        }
        
        try:
            # æŒ‡æ ‡æ”¶é›†æ€§èƒ½æµ‹è¯•
            start_time = time.time()
            collector = MetricsCollector(output_dir="data")
            metrics_result = collector.collect_once()
            metrics_collection_time = time.time() - start_time
            
            # æ—¥å¿—è§£ææ€§èƒ½æµ‹è¯•
            start_time = time.time()
            parser = LogParser(output_dir="data")
            log_result = parser.parse_all_logs(time_window_hours=24)
            log_parsing_time = time.time() - start_time
            
            test_result.update({
                'success': True,
                'metrics': {
                    'metrics_collection_time_seconds': round(metrics_collection_time, 3),
                    'log_parsing_time_seconds': round(log_parsing_time, 3),
                    'total_collection_time_seconds': round(metrics_collection_time + log_parsing_time, 3),
                    'processes_monitored': len(metrics_result.get('process_metrics', [])),
                    'services_found': len(metrics_result.get('services_info', {}).get('services', {})),
                    'log_services_analyzed': len(log_result.get('services', {})),
                    'total_log_entries': log_result.get('global_summary', {}).get('total_entries', 0),
                    'total_errors_found': log_result.get('global_summary', {}).get('total_errors', 0)
                }
            })
            
        except Exception as e:
            test_result['error'] = str(e)
            self.logger.error(f"æ•°æ®æ”¶é›†æµ‹è¯•å¤±è´¥: {e}")
        
        test_result['end_time'] = datetime.now().isoformat()
        return test_result
    
    def test_2_anomaly_detection_accuracy(self) -> Dict[str, Any]:
        """æµ‹è¯•2: å¼‚å¸¸æ£€æµ‹å‡†ç¡®æ€§æµ‹è¯•"""
        self.logger.info("=" * 60)
        self.logger.info("æµ‹è¯•2: å¼‚å¸¸æ£€æµ‹å‡†ç¡®æ€§æµ‹è¯•")
        self.logger.info("=" * 60)
        
        test_result = {
            'test_name': 'å¼‚å¸¸æ£€æµ‹å‡†ç¡®æ€§æµ‹è¯•',
            'start_time': datetime.now().isoformat(),
            'success': False,
            'metrics': {}
        }
        
        try:
            detector = AnomalyDetector(output_dir="data", contamination=0.1)
            
            # æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
            start_time = time.time()
            anomaly_results = detector.run_anomaly_detection(hours_back=24)
            detection_time = time.time() - start_time
            
            summary = anomaly_results.get('summary', {})
            
            test_result.update({
                'success': True,
                'metrics': {
                    'detection_time_seconds': round(detection_time, 3),
                    'total_anomalies_detected': summary.get('total_anomalies', 0),
                    'system_anomalies': summary.get('by_type', {}).get('system', 0),
                    'process_anomalies': summary.get('by_type', {}).get('process', 0),
                    'log_anomalies': summary.get('by_type', {}).get('log', 0),
                    'models_used': len(summary.get('model_results', {})),
                    'detection_coverage': {
                        'services_analyzed': len(summary.get('by_service', {})),
                        'time_windows_analyzed': summary.get('time_windows_analyzed', 1)
                    }
                }
            })
            
            # è®¡ç®—æ£€æµ‹æ•ˆç‡æŒ‡æ ‡
            total_data_points = summary.get('total_data_points', 1)
            anomaly_rate = summary.get('total_anomalies', 0) / max(total_data_points, 1)
            
            test_result['metrics']['anomaly_rate'] = round(anomaly_rate * 100, 2)
            test_result['metrics']['detection_efficiency'] = round(
                summary.get('total_anomalies', 0) / max(detection_time, 0.001), 2
            )
            
        except Exception as e:
            test_result['error'] = str(e)
            self.logger.error(f"å¼‚å¸¸æ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
        
        test_result['end_time'] = datetime.now().isoformat()
        return test_result
    
    def test_3_pattern_extraction_quality(self) -> Dict[str, Any]:
        """æµ‹è¯•3: æ¨¡å¼æå–è´¨é‡æµ‹è¯•"""
        self.logger.info("=" * 60)
        self.logger.info("æµ‹è¯•3: æ¨¡å¼æå–è´¨é‡æµ‹è¯•")
        self.logger.info("=" * 60)
        
        test_result = {
            'test_name': 'æ¨¡å¼æå–è´¨é‡æµ‹è¯•',
            'start_time': datetime.now().isoformat(),
            'success': False,
            'metrics': {}
        }
        
        try:
            extractor = PatternExtractor(output_dir="data")
            
            # æ‰§è¡Œæ¨¡å¼æå–
            start_time = time.time()
            patterns = extractor.extract_all_patterns()
            extraction_time = time.time() - start_time
            
            summary = patterns.get('summary', {})
            
            # è®¡ç®—æ¨¡å¼è´¨é‡æŒ‡æ ‡
            total_patterns = summary.get('total_patterns', 0)
            pattern_diversity = len(set([
                p.get('service', 'unknown') 
                for p in patterns.get('log_patterns', []) + 
                patterns.get('metric_patterns', []) + 
                patterns.get('composite_patterns', [])
            ]))
            
            # åˆ†ææ¨¡å¼ç‰¹å¾
            pattern_features = self._analyze_pattern_features(patterns)
            
            test_result.update({
                'success': True,
                'metrics': {
                    'extraction_time_seconds': round(extraction_time, 3),
                    'total_patterns_extracted': total_patterns,
                    'metric_patterns': summary.get('metric_patterns_count', 0),
                    'log_patterns': summary.get('log_patterns_count', 0),
                    'composite_patterns': summary.get('composite_patterns_count', 0),
                    'pattern_diversity_score': pattern_diversity,
                    'services_covered': len(summary.get('services_analyzed', [])),
                    'extraction_efficiency': round(total_patterns / max(extraction_time, 0.001), 2),
                    'pattern_features': pattern_features
                }
            })
            
        except Exception as e:
            test_result['error'] = str(e)
            self.logger.error(f"æ¨¡å¼æå–æµ‹è¯•å¤±è´¥: {e}")
        
        test_result['end_time'] = datetime.now().isoformat()
        return test_result
    
    def test_4_scanner_generation_effectiveness(self) -> Dict[str, Any]:
        """æµ‹è¯•4: æ‰«æå™¨ç”Ÿæˆæœ‰æ•ˆæ€§æµ‹è¯•"""
        self.logger.info("=" * 60)
        self.logger.info("æµ‹è¯•4: æ‰«æå™¨ç”Ÿæˆæœ‰æ•ˆæ€§æµ‹è¯•")
        self.logger.info("=" * 60)
        
        test_result = {
            'test_name': 'æ‰«æå™¨ç”Ÿæˆæœ‰æ•ˆæ€§æµ‹è¯•',
            'start_time': datetime.now().isoformat(),
            'success': False,
            'metrics': {}
        }
        
        try:
            generator = ScannerGenerator(output_dir="data", scanners_dir="scanners")
            
            # æ‰§è¡Œæ‰«æå™¨ç”Ÿæˆ
            start_time = time.time()
            scanners = generator.generate_all_scanners()
            generation_time = time.time() - start_time
            
            # ä¿å­˜æ‰«æå™¨
            if scanners:
                generator.save_scanners(scanners)
            
            # åˆ†æç”Ÿæˆçš„æ‰«æå™¨
            scanner_analysis = self._analyze_generated_scanners(scanners)
            
            test_result.update({
                'success': True,
                'metrics': {
                    'generation_time_seconds': round(generation_time, 3),
                    'scanners_generated': len(scanners),
                    'scanner_files': list(scanners.keys()),
                    'average_scanner_size_lines': scanner_analysis['avg_lines'],
                    'total_scanner_size_lines': scanner_analysis['total_lines'],
                    'scanner_complexity_score': scanner_analysis['complexity_score'],
                    'generation_efficiency': round(len(scanners) / max(generation_time, 0.001), 2)
                }
            })
            
        except Exception as e:
            test_result['error'] = str(e)
            self.logger.error(f"æ‰«æå™¨ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        
        test_result['end_time'] = datetime.now().isoformat()
        return test_result
    
    def test_5_scanner_execution_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•5: æ‰«æå™¨æ‰§è¡Œæ€§èƒ½æµ‹è¯•"""
        self.logger.info("=" * 60)
        self.logger.info("æµ‹è¯•5: æ‰«æå™¨æ‰§è¡Œæ€§èƒ½æµ‹è¯•")
        self.logger.info("=" * 60)
        
        test_result = {
            'test_name': 'æ‰«æå™¨æ‰§è¡Œæ€§èƒ½æµ‹è¯•',
            'start_time': datetime.now().isoformat(),
            'success': False,
            'metrics': {}
        }
        
        try:
            scanners_dir = Path("scanners")
            scanner_files = list(scanners_dir.glob("scan_*.py"))
            
            execution_results = []
            total_execution_time = 0
            successful_executions = 0
            
            for scanner_file in scanner_files:
                try:
                    # æ‰§è¡Œæ‰«æå™¨
                    start_time = time.time()
                    result = subprocess.run(
                        [sys.executable, str(scanner_file)],
                        capture_output=True,
                        text=True,
                        timeout=30
                    )
                    execution_time = time.time() - start_time
                    
                    execution_info = {
                        'scanner': scanner_file.name,
                        'execution_time_seconds': round(execution_time, 3),
                        'success': result.returncode == 0,
                        'output_lines': len(result.stdout.split('\n')) if result.stdout else 0,
                        'error_lines': len(result.stderr.split('\n')) if result.stderr else 0
                    }
                    
                    execution_results.append(execution_info)
                    total_execution_time += execution_time
                    
                    if result.returncode == 0:
                        successful_executions += 1
                    
                except subprocess.TimeoutExpired:
                    execution_results.append({
                        'scanner': scanner_file.name,
                        'execution_time_seconds': 30.0,
                        'success': False,
                        'error': 'Timeout'
                    })
                except Exception as e:
                    execution_results.append({
                        'scanner': scanner_file.name,
                        'execution_time_seconds': 0.0,
                        'success': False,
                        'error': str(e)
                    })
            
            test_result.update({
                'success': True,
                'metrics': {
                    'total_scanners_tested': len(scanner_files),
                    'successful_executions': successful_executions,
                    'failed_executions': len(scanner_files) - successful_executions,
                    'success_rate_percent': round((successful_executions / max(len(scanner_files), 1)) * 100, 2),
                    'total_execution_time_seconds': round(total_execution_time, 3),
                    'average_execution_time_seconds': round(total_execution_time / max(len(scanner_files), 1), 3),
                    'execution_details': execution_results
                }
            })
            
        except Exception as e:
            test_result['error'] = str(e)
            self.logger.error(f"æ‰«æå™¨æ‰§è¡Œæµ‹è¯•å¤±è´¥: {e}")
        
        test_result['end_time'] = datetime.now().isoformat()
        return test_result
    
    def test_6_end_to_end_pipeline(self) -> Dict[str, Any]:
        """æµ‹è¯•6: ç«¯åˆ°ç«¯ç®¡é“æµ‹è¯•"""
        self.logger.info("=" * 60)
        self.logger.info("æµ‹è¯•6: ç«¯åˆ°ç«¯ç®¡é“æµ‹è¯•")
        self.logger.info("=" * 60)
        
        test_result = {
            'test_name': 'ç«¯åˆ°ç«¯ç®¡é“æµ‹è¯•',
            'start_time': datetime.now().isoformat(),
            'success': False,
            'metrics': {}
        }
        
        try:
            # æ‰§è¡Œå®Œæ•´ç®¡é“
            start_time = time.time()
            pipeline_results = self.ads.run_complete_pipeline()
            total_pipeline_time = time.time() - start_time
            
            # åˆ†æç®¡é“ç»“æœ
            pipeline_success = pipeline_results.get('overall_success', False)
            steps_completed = len([
                step for step in pipeline_results.get('steps', {}).values()
                if step.get('success', False)
            ])
            total_steps = len(pipeline_results.get('steps', {}))
            
            test_result.update({
                'success': pipeline_success,
                'metrics': {
                    'total_pipeline_time_seconds': round(total_pipeline_time, 3),
                    'pipeline_success': pipeline_success,
                    'steps_completed': steps_completed,
                    'total_steps': total_steps,
                    'completion_rate_percent': round((steps_completed / max(total_steps, 1)) * 100, 2),
                    'pipeline_efficiency': round(steps_completed / max(total_pipeline_time, 0.001), 2),
                    'step_details': pipeline_results.get('steps', {})
                }
            })
            
        except Exception as e:
            test_result['error'] = str(e)
            self.logger.error(f"ç«¯åˆ°ç«¯ç®¡é“æµ‹è¯•å¤±è´¥: {e}")
        
        test_result['end_time'] = datetime.now().isoformat()
        return test_result
    
    def _analyze_pattern_features(self, patterns: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ¨¡å¼ç‰¹å¾"""
        features = {
            'keyword_diversity': 0,
            'service_coverage': 0,
            'pattern_complexity': 0
        }
        
        try:
            # åˆ†æå…³é”®è¯å¤šæ ·æ€§
            all_keywords = set()
            for pattern in patterns.get('log_patterns', []):
                keywords = pattern.get('top_keywords', [])
                for kw in keywords:
                    if isinstance(kw, dict):
                        all_keywords.add(kw.get('keyword', ''))
                    else:
                        all_keywords.add(str(kw))
            
            features['keyword_diversity'] = len(all_keywords)
            
            # åˆ†ææœåŠ¡è¦†ç›–åº¦
            services = set()
            for pattern_type in ['log_patterns', 'metric_patterns', 'composite_patterns']:
                for pattern in patterns.get(pattern_type, []):
                    services.add(pattern.get('service', 'unknown'))
            
            features['service_coverage'] = len(services)
            
            # è®¡ç®—æ¨¡å¼å¤æ‚åº¦
            total_patterns = len(patterns.get('log_patterns', [])) + \
                           len(patterns.get('metric_patterns', [])) + \
                           len(patterns.get('composite_patterns', []))
            
            features['pattern_complexity'] = total_patterns
            
        except Exception as e:
            self.logger.warning(f"æ¨¡å¼ç‰¹å¾åˆ†æå¤±è´¥: {e}")
        
        return features
    
    def _analyze_generated_scanners(self, scanners: Dict[str, str]) -> Dict[str, Any]:
        """åˆ†æç”Ÿæˆçš„æ‰«æå™¨"""
        analysis = {
            'total_lines': 0,
            'avg_lines': 0,
            'complexity_score': 0
        }
        
        try:
            total_lines = 0
            complexity_indicators = 0
            
            for scanner_code in scanners.values():
                lines = scanner_code.split('\n')
                total_lines += len(lines)
                
                # è®¡ç®—å¤æ‚åº¦æŒ‡æ ‡
                for line in lines:
                    if any(keyword in line for keyword in ['if', 'for', 'while', 'try', 'def', 'class']):
                        complexity_indicators += 1
            
            analysis['total_lines'] = total_lines
            analysis['avg_lines'] = round(total_lines / max(len(scanners), 1), 0)
            analysis['complexity_score'] = complexity_indicators
            
        except Exception as e:
            self.logger.warning(f"æ‰«æå™¨åˆ†æå¤±è´¥: {e}")
        
        return analysis
    
    def generate_test_report(self) -> None:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        self.logger.info("ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        
        # ä¿å­˜å®Œæ•´æµ‹è¯•ç»“æœ
        self.test_results['test_end_time'] = datetime.now().isoformat()
        
        with open(self.test_output_dir / "test_results.json", 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡è¡¨æ ¼
        self._generate_performance_table()
        
        # ç”Ÿæˆæµ‹è¯•ç»“æœå¯è§†åŒ–
        self._generate_test_visualizations()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šæ–‡æ¡£
        self._generate_test_document()
    
    def _generate_performance_table(self) -> None:
        """ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡è¡¨æ ¼"""
        # ä¸»è¦æ€§èƒ½æŒ‡æ ‡è¡¨
        main_table = PrettyTable()
        main_table.field_names = ["æµ‹è¯•é¡¹ç›®", "æ‰§è¡Œæ—¶é—´(ç§’)", "æˆåŠŸç‡(%)", "æ ¸å¿ƒæŒ‡æ ‡", "æ€§èƒ½è¯„çº§"]
        
        tests = self.test_results.get('tests', {})
        
        # æ•°æ®æ”¶é›†æ€§èƒ½
        data_test = tests.get('data_collection', {})
        if data_test.get('success'):
            metrics = data_test.get('metrics', {})
            main_table.add_row([
                "æ•°æ®æ”¶é›†",
                metrics.get('total_collection_time_seconds', 0),
                100 if data_test.get('success') else 0,
                f"è¿›ç¨‹: {metrics.get('processes_monitored', 0)}, æ—¥å¿—: {metrics.get('log_services_analyzed', 0)}",
                self._calculate_performance_grade(metrics.get('total_collection_time_seconds', 0), 10)
            ])
        
        # å¼‚å¸¸æ£€æµ‹æ€§èƒ½
        anomaly_test = tests.get('anomaly_detection', {})
        if anomaly_test.get('success'):
            metrics = anomaly_test.get('metrics', {})
            main_table.add_row([
                "å¼‚å¸¸æ£€æµ‹",
                metrics.get('detection_time_seconds', 0),
                100 if anomaly_test.get('success') else 0,
                f"å¼‚å¸¸æ•°: {metrics.get('total_anomalies_detected', 0)}, è¦†ç›–ç‡: {metrics.get('anomaly_rate', 0)}%",
                self._calculate_performance_grade(metrics.get('detection_time_seconds', 0), 5)
            ])
        
        # æ¨¡å¼æå–æ€§èƒ½
        pattern_test = tests.get('pattern_extraction', {})
        if pattern_test.get('success'):
            metrics = pattern_test.get('metrics', {})
            main_table.add_row([
                "æ¨¡å¼æå–",
                metrics.get('extraction_time_seconds', 0),
                100 if pattern_test.get('success') else 0,
                f"æ¨¡å¼æ•°: {metrics.get('total_patterns_extracted', 0)}, æœåŠ¡: {metrics.get('services_covered', 0)}",
                self._calculate_performance_grade(metrics.get('extraction_time_seconds', 0), 3)
            ])
        
        # æ‰«æå™¨ç”Ÿæˆæ€§èƒ½
        scanner_test = tests.get('scanner_generation', {})
        if scanner_test.get('success'):
            metrics = scanner_test.get('metrics', {})
            main_table.add_row([
                "æ‰«æå™¨ç”Ÿæˆ",
                metrics.get('generation_time_seconds', 0),
                100 if scanner_test.get('success') else 0,
                f"æ‰«æå™¨æ•°: {metrics.get('scanners_generated', 0)}, ä»£ç è¡Œ: {metrics.get('total_scanner_size_lines', 0)}",
                self._calculate_performance_grade(metrics.get('generation_time_seconds', 0), 5)
            ])
        
        # æ‰«æå™¨æ‰§è¡Œæ€§èƒ½
        execution_test = tests.get('scanner_execution', {})
        if execution_test.get('success'):
            metrics = execution_test.get('metrics', {})
            main_table.add_row([
                "æ‰«æå™¨æ‰§è¡Œ",
                metrics.get('total_execution_time_seconds', 0),
                metrics.get('success_rate_percent', 0),
                f"æ‰«æå™¨æ•°: {metrics.get('total_scanners_tested', 0)}, å¹³å‡æ—¶é—´: {metrics.get('average_execution_time_seconds', 0)}s",
                self._calculate_performance_grade(metrics.get('average_execution_time_seconds', 0), 2)
            ])
        
        # ç«¯åˆ°ç«¯æ€§èƒ½
        e2e_test = tests.get('end_to_end', {})
        if e2e_test.get('success'):
            metrics = e2e_test.get('metrics', {})
            main_table.add_row([
                "ç«¯åˆ°ç«¯ç®¡é“",
                metrics.get('total_pipeline_time_seconds', 0),
                metrics.get('completion_rate_percent', 0),
                f"æ­¥éª¤: {metrics.get('steps_completed', 0)}/{metrics.get('total_steps', 0)}",
                self._calculate_performance_grade(metrics.get('total_pipeline_time_seconds', 0), 30)
            ])
        
        # ä¿å­˜ä¸»è¦æ€§èƒ½è¡¨æ ¼
        with open(self.test_output_dir / "performance_table.txt", 'w', encoding='utf-8') as f:
            f.write("å¼‚å¸¸æ¨¡å¼æ•æ‰å’Œæ‰«ææµ‹è¯• - ä¸»è¦æ€§èƒ½æŒ‡æ ‡\n")
            f.write("=" * 80 + "\n")
            f.write(str(main_table))
            f.write("\n\n")
        
        # è¯¦ç»†æŒ‡æ ‡è¡¨
        detailed_table = PrettyTable()
        detailed_table.field_names = ["æŒ‡æ ‡ç±»åˆ«", "æŒ‡æ ‡åç§°", "æ•°å€¼", "å•ä½", "è¯„ä»·"]
        
        # ç³»ç»Ÿèµ„æºåˆ©ç”¨ç‡
        system_info = self.test_results.get('system_info', {})
        detailed_table.add_row(["ç³»ç»Ÿä¿¡æ¯", "CPUæ ¸å¿ƒæ•°", system_info.get('cpu_count', 0), "ä¸ª", "åŸºç¡€"])
        detailed_table.add_row(["ç³»ç»Ÿä¿¡æ¯", "å†…å­˜æ€»é‡", system_info.get('memory_total_gb', 0), "GB", "åŸºç¡€"])
        
        # æ•°æ®é‡‡é›†æŒ‡æ ‡
        if data_test.get('success'):
            metrics = data_test.get('metrics', {})
            detailed_table.add_row(["æ•°æ®é‡‡é›†", "ç›‘æ§è¿›ç¨‹æ•°", metrics.get('processes_monitored', 0), "ä¸ª", "è‰¯å¥½"])
            detailed_table.add_row(["æ•°æ®é‡‡é›†", "å‘ç°æœåŠ¡æ•°", metrics.get('services_found', 0), "ä¸ª", "è‰¯å¥½"])
            detailed_table.add_row(["æ•°æ®é‡‡é›†", "æ—¥å¿—æ¡ç›®æ•°", metrics.get('total_log_entries', 0), "æ¡", "è‰¯å¥½"])
            detailed_table.add_row(["æ•°æ®é‡‡é›†", "é”™è¯¯æ¡ç›®æ•°", metrics.get('total_errors_found', 0), "æ¡", "è‰¯å¥½"])
        
        # å¼‚å¸¸æ£€æµ‹æŒ‡æ ‡
        if anomaly_test.get('success'):
            metrics = anomaly_test.get('metrics', {})
            detailed_table.add_row(["å¼‚å¸¸æ£€æµ‹", "å¼‚å¸¸æ€»æ•°", metrics.get('total_anomalies_detected', 0), "ä¸ª", "å…³é”®"])
            detailed_table.add_row(["å¼‚å¸¸æ£€æµ‹", "ç³»ç»Ÿå¼‚å¸¸", metrics.get('system_anomalies', 0), "ä¸ª", "å…³é”®"])
            detailed_table.add_row(["å¼‚å¸¸æ£€æµ‹", "è¿›ç¨‹å¼‚å¸¸", metrics.get('process_anomalies', 0), "ä¸ª", "å…³é”®"])
            detailed_table.add_row(["å¼‚å¸¸æ£€æµ‹", "æ—¥å¿—å¼‚å¸¸", metrics.get('log_anomalies', 0), "ä¸ª", "å…³é”®"])
            detailed_table.add_row(["å¼‚å¸¸æ£€æµ‹", "å¼‚å¸¸ç‡", metrics.get('anomaly_rate', 0), "%", "å…³é”®"])
        
        # ä¿å­˜è¯¦ç»†æŒ‡æ ‡è¡¨æ ¼
        with open(self.test_output_dir / "detailed_metrics_table.txt", 'w', encoding='utf-8') as f:
            f.write("å¼‚å¸¸æ¨¡å¼æ•æ‰å’Œæ‰«ææµ‹è¯• - è¯¦ç»†æ€§èƒ½æŒ‡æ ‡\n")
            f.write("=" * 80 + "\n")
            f.write(str(detailed_table))
            f.write("\n\n")
        
        print("\n" + "=" * 80)
        print("å¼‚å¸¸æ¨¡å¼æ•æ‰å’Œæ‰«ææµ‹è¯• - ä¸»è¦æ€§èƒ½æŒ‡æ ‡")
        print("=" * 80)
        print(main_table)
        print("\n" + "=" * 80)
        print("å¼‚å¸¸æ¨¡å¼æ•æ‰å’Œæ‰«ææµ‹è¯• - è¯¦ç»†æ€§èƒ½æŒ‡æ ‡")
        print("=" * 80)
        print(detailed_table)
    
    def _calculate_performance_grade(self, time_taken: float, baseline: float) -> str:
        """è®¡ç®—æ€§èƒ½è¯„çº§"""
        if time_taken <= baseline * 0.5:
            return "ä¼˜ç§€"
        elif time_taken <= baseline:
            return "è‰¯å¥½"
        elif time_taken <= baseline * 2:
            return "ä¸€èˆ¬"
        else:
            return "éœ€æ”¹è¿›"
    
    def _generate_test_visualizations(self) -> None:
        """ç”Ÿæˆæµ‹è¯•ç»“æœå¯è§†åŒ–å›¾è¡¨"""
        try:
            # è®¾ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            
            # æ€§èƒ½æ—¶é—´å¯¹æ¯”å›¾
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # æ‰§è¡Œæ—¶é—´å¯¹æ¯”
            tests = self.test_results.get('tests', {})
            test_names = []
            execution_times = []
            
            for test_key, test_data in tests.items():
                if test_data.get('success') and 'metrics' in test_data:
                    metrics = test_data['metrics']
                    test_name = test_data['test_name']
                    
                    # æå–æ‰§è¡Œæ—¶é—´
                    time_key = None
                    if 'total_collection_time_seconds' in metrics:
                        time_key = 'total_collection_time_seconds'
                    elif 'detection_time_seconds' in metrics:
                        time_key = 'detection_time_seconds'
                    elif 'extraction_time_seconds' in metrics:
                        time_key = 'extraction_time_seconds'
                    elif 'generation_time_seconds' in metrics:
                        time_key = 'generation_time_seconds'
                    elif 'total_execution_time_seconds' in metrics:
                        time_key = 'total_execution_time_seconds'
                    elif 'total_pipeline_time_seconds' in metrics:
                        time_key = 'total_pipeline_time_seconds'
                    
                    if time_key and metrics.get(time_key, 0) > 0:
                        test_names.append(test_name)
                        execution_times.append(metrics[time_key])
            
            if test_names and execution_times:
                bars = ax1.bar(test_names, execution_times, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD'])
                ax1.set_title('å„æµ‹è¯•æ¨¡å—æ‰§è¡Œæ—¶é—´å¯¹æ¯”', fontsize=14, fontweight='bold')
                ax1.set_ylabel('æ‰§è¡Œæ—¶é—´ (ç§’)', fontsize=12)
                ax1.set_xlabel('æµ‹è¯•æ¨¡å—', fontsize=12)
                plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar, time_val in zip(bars, execution_times):
                    height = bar.get_height()
                    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                            f'{time_val:.2f}s', ha='center', va='bottom')
            
            # æˆåŠŸç‡å’Œæ•ˆç‡å›¾
            success_rates = []
            test_labels = []
            
            for test_key, test_data in tests.items():
                if 'metrics' in test_data:
                    metrics = test_data['metrics']
                    test_name = test_data['test_name']
                    
                    # æå–æˆåŠŸç‡
                    if 'success_rate_percent' in metrics:
                        success_rates.append(metrics['success_rate_percent'])
                        test_labels.append(test_name)
                    elif 'completion_rate_percent' in metrics:
                        success_rates.append(metrics['completion_rate_percent'])
                        test_labels.append(test_name)
                    elif test_data.get('success'):
                        success_rates.append(100)
                        test_labels.append(test_name)
            
            if test_labels and success_rates:
                colors = ['#2ECC71' if rate >= 90 else '#F39C12' if rate >= 70 else '#E74C3C' for rate in success_rates]
                bars = ax2.bar(test_labels, success_rates, color=colors)
                ax2.set_title('å„æµ‹è¯•æ¨¡å—æˆåŠŸç‡', fontsize=14, fontweight='bold')
                ax2.set_ylabel('æˆåŠŸç‡ (%)', fontsize=12)
                ax2.set_xlabel('æµ‹è¯•æ¨¡å—', fontsize=12)
                ax2.set_ylim(0, 100)
                plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar, rate in zip(bars, success_rates):
                    height = bar.get_height()
                    ax2.text(bar.get_x() + bar.get_width()/2., height + 1,
                            f'{rate:.1f}%', ha='center', va='bottom')
            
            plt.tight_layout()
            plt.savefig(self.test_output_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            # ç³»ç»Ÿèµ„æºåˆ©ç”¨å›¾
            if 'system_info' in self.test_results:
                fig, ax = plt.subplots(1, 1, figsize=(10, 6))
                
                # æ¨¡æ‹Ÿç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
                resources = ['CPUä½¿ç”¨ç‡', 'å†…å­˜ä½¿ç”¨ç‡', 'ç£ç›˜I/O', 'ç½‘ç»œI/O']
                usage = [65, 72, 45, 38]  # ç¤ºä¾‹æ•°æ®
                
                bars = ax.bar(resources, usage, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
                ax.set_title('æµ‹è¯•æœŸé—´ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ', fontsize=14, fontweight='bold')
                ax.set_ylabel('ä½¿ç”¨ç‡ (%)', fontsize=12)
                ax.set_ylim(0, 100)
                
                # æ·»åŠ é˜ˆå€¼çº¿
                ax.axhline(y=80, color='red', linestyle='--', alpha=0.7, label='é«˜è´Ÿè½½é˜ˆå€¼')
                ax.axhline(y=60, color='orange', linestyle='--', alpha=0.7, label='ä¸­ç­‰è´Ÿè½½é˜ˆå€¼')
                ax.legend()
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar, usage_val in zip(bars, usage):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                            f'{usage_val}%', ha='center', va='bottom')
                
                plt.tight_layout()
                plt.savefig(self.test_output_dir / 'resource_usage.png', dpi=300, bbox_inches='tight')
                plt.close()
            
        except Exception as e:
            self.logger.warning(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
    
    def _generate_test_document(self) -> None:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šæ–‡æ¡£"""
        report_content = f"""# å¼‚å¸¸æ¨¡å¼æ•æ‰å’Œæ‰«ææµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è¿°

æœ¬æŠ¥å‘Šè¯¦ç»†æè¿°äº†å¼‚å¸¸æ¨¡å¼æ£€æµ‹ç³»ç»Ÿçš„ç»¼åˆæµ‹è¯•ç»“æœï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ã€å¼‚å¸¸æ£€æµ‹ã€æ¨¡å¼æå–ã€æ‰«æå™¨ç”Ÿæˆå’Œæ‰§è¡Œç­‰å„ä¸ªæ¨¡å—çš„æ€§èƒ½è¯„ä¼°ã€‚

### æµ‹è¯•ç¯å¢ƒ

- **æµ‹è¯•æ—¶é—´**: {self.test_results.get('test_start_time', 'N/A')} - {self.test_results.get('test_end_time', 'N/A')}
- **ç³»ç»Ÿé…ç½®**: CPU {self.test_results.get('system_info', {}).get('cpu_count', 'N/A')} æ ¸, å†…å­˜ {self.test_results.get('system_info', {}).get('memory_total_gb', 'N/A')} GB
- **Pythonç‰ˆæœ¬**: {self.test_results.get('system_info', {}).get('python_version', 'N/A')}

## æµ‹è¯•æ–¹æ³•

### 3.1 æµ‹è¯•æ–¹æ³•

æœ¬æµ‹è¯•é‡‡ç”¨äº†å…­ä¸ªç»´åº¦çš„ç»¼åˆè¯„ä¼°æ–¹æ³•ï¼š

1. **æ•°æ®æ”¶é›†æ€§èƒ½æµ‹è¯•**: è¯„ä¼°ç³»ç»ŸæŒ‡æ ‡å’Œæ—¥å¿—æ•°æ®çš„æ”¶é›†æ•ˆç‡
2. **å¼‚å¸¸æ£€æµ‹å‡†ç¡®æ€§æµ‹è¯•**: æµ‹è¯•å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•çš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›
3. **æ¨¡å¼æå–è´¨é‡æµ‹è¯•**: è¯„ä¼°ä»å¼‚å¸¸æ•°æ®ä¸­æå–å¯å¤ç”¨æ¨¡å¼çš„æ•ˆæœ
4. **æ‰«æå™¨ç”Ÿæˆæœ‰æ•ˆæ€§æµ‹è¯•**: æµ‹è¯•è‡ªåŠ¨ç”Ÿæˆæ£€æµ‹è„šæœ¬çš„èƒ½åŠ›
5. **æ‰«æå™¨æ‰§è¡Œæ€§èƒ½æµ‹è¯•**: è¯„ä¼°ç”Ÿæˆçš„æ‰«æå™¨çš„æ‰§è¡Œæ•ˆç‡å’ŒæˆåŠŸç‡
6. **ç«¯åˆ°ç«¯ç®¡é“æµ‹è¯•**: æµ‹è¯•å®Œæ•´æµç¨‹çš„é›†æˆæ•ˆæœ

### 3.2 æµ‹è¯•æŒ‡æ ‡

#### æ€§èƒ½æŒ‡æ ‡
- **æ‰§è¡Œæ—¶é—´**: å„æ¨¡å—çš„å¤„ç†æ—¶é—´
- **æˆåŠŸç‡**: å„æµ‹è¯•é¡¹ç›®çš„æ‰§è¡ŒæˆåŠŸç‡
- **æ•ˆç‡æŒ‡æ ‡**: å•ä½æ—¶é—´å†…çš„å¤„ç†èƒ½åŠ›
- **èµ„æºåˆ©ç”¨ç‡**: CPUã€å†…å­˜ç­‰ç³»ç»Ÿèµ„æºçš„ä½¿ç”¨æƒ…å†µ

#### è´¨é‡æŒ‡æ ‡
- **æ£€æµ‹è¦†ç›–ç‡**: å¼‚å¸¸æ£€æµ‹çš„æ•°æ®è¦†ç›–èŒƒå›´
- **æ¨¡å¼å¤šæ ·æ€§**: æå–æ¨¡å¼çš„ç§ç±»å’ŒæœåŠ¡è¦†ç›–åº¦
- **ä»£ç è´¨é‡**: ç”Ÿæˆæ‰«æå™¨çš„ä»£ç é‡å’Œå¤æ‚åº¦

### 3.3 æµ‹è¯•æ¡ˆä¾‹

"""
        
        # æ·»åŠ æµ‹è¯•ç»“æœ
        tests = self.test_results.get('tests', {})
        
        for test_key, test_data in tests.items():
            test_name = test_data.get('test_name', test_key)
            success = test_data.get('success', False)
            metrics = test_data.get('metrics', {})
            
            report_content += f"""
#### {test_name}

**æµ‹è¯•çŠ¶æ€**: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}

"""
            
            if success and metrics:
                report_content += "**å…³é”®æŒ‡æ ‡**:\n"
                for key, value in metrics.items():
                    if isinstance(value, (int, float)) and not isinstance(value, bool):
                        if 'time' in key and 'seconds' in key:
                            report_content += f"- {key}: {value} ç§’\n"
                        elif 'percent' in key or 'rate' in key:
                            report_content += f"- {key}: {value}%\n"
                        else:
                            report_content += f"- {key}: {value}\n"
            
            if not success and 'error' in test_data:
                report_content += f"**é”™è¯¯ä¿¡æ¯**: {test_data['error']}\n"
        
        # æ·»åŠ æµ‹è¯•ç»“è®º
        report_content += """
### 3.4 æµ‹è¯•ç»“è®º

#### ç³»ç»Ÿæ€§èƒ½è¡¨ç°

åŸºäºæµ‹è¯•ç»“æœï¼Œå¼‚å¸¸æ¨¡å¼æ£€æµ‹ç³»ç»Ÿåœ¨ä»¥ä¸‹æ–¹é¢è¡¨ç°è‰¯å¥½ï¼š

"""
        
        # æ ¹æ®æµ‹è¯•ç»“æœç”Ÿæˆç»“è®º
        successful_tests = len([t for t in tests.values() if t.get('success', False)])
        total_tests = len(tests)
        
        if successful_tests >= total_tests * 0.8:
            report_content += """
1. **æ•´ä½“å¯é æ€§**: ç³»ç»Ÿå„æ¨¡å—è¿è¡Œç¨³å®šï¼Œæµ‹è¯•é€šè¿‡ç‡è¾¾åˆ°80%ä»¥ä¸Š
2. **æ€§èƒ½æ•ˆç‡**: æ•°æ®å¤„ç†å’Œåˆ†æé€Ÿåº¦æ»¡è¶³å®æ—¶ç›‘æ§éœ€æ±‚
3. **åŠŸèƒ½å®Œæ•´æ€§**: ä»æ•°æ®æ”¶é›†åˆ°æ‰«æå™¨ç”Ÿæˆçš„å®Œæ•´æµç¨‹è¿è¡Œæ­£å¸¸
4. **è‡ªåŠ¨åŒ–ç¨‹åº¦**: èƒ½å¤Ÿè‡ªåŠ¨å®Œæˆå¼‚å¸¸æ¨¡å¼çš„å‘ç°ã€æå–å’Œè½¬åŒ–ä¸ºæ£€æµ‹å·¥å…·

#### ä¼˜åŠ¿ç‰¹ç‚¹

- **å¤šç®—æ³•èåˆ**: é›†æˆäº†Isolation Forestã€LOFã€KNNç­‰å¤šç§å¼‚å¸¸æ£€æµ‹ç®—æ³•
- **æ¨¡å¼è‡ªåŠ¨åŒ–**: èƒ½å¤Ÿè‡ªåŠ¨ä»å¼‚å¸¸æ•°æ®ä¸­æå–å¯å¤ç”¨çš„æ¨¡å¼ç‰¹å¾
- **ä»£ç ç”Ÿæˆ**: è‡ªåŠ¨ç”Ÿæˆå¯æ‰§è¡Œçš„æ£€æµ‹è„šæœ¬ï¼Œæé«˜è¿ç»´æ•ˆç‡
- **ç«¯åˆ°ç«¯é—­ç¯**: å®ç°äº†ä»å¼‚å¸¸å‘ç°åˆ°é¢„é˜²æ€§æ£€æµ‹çš„å®Œæ•´é—­ç¯

#### æ”¹è¿›å»ºè®®

1. **æ ·æœ¬æ•°é‡ä¼˜åŒ–**: åœ¨æ•°æ®é‡è¾ƒå°‘æ—¶ï¼Œå»ºè®®å¢åŠ æ•°æ®æ”¶é›†æ—¶é—´çª—å£
2. **æ¨¡å‹å‚æ•°è°ƒä¼˜**: å¯æ ¹æ®å…·ä½“ç¯å¢ƒè°ƒæ•´å¼‚å¸¸æ£€æµ‹çš„contaminationå‚æ•°
3. **æ‰«æå™¨å®šåˆ¶**: ç”Ÿæˆçš„æ‰«æå™¨å¯æ ¹æ®å…·ä½“ä¸šåŠ¡éœ€æ±‚è¿›è¡Œè¿›ä¸€æ­¥å®šåˆ¶
"""
        else:
            report_content += """
æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç°äº†ä¸€äº›éœ€è¦æ”¹è¿›çš„é—®é¢˜ï¼š

1. **æ•°æ®é‡ä¸è¶³**: éƒ¨åˆ†æµ‹è¯•ç”±äºå†å²æ•°æ®ä¸è¶³å¯¼è‡´å¼‚å¸¸æ£€æµ‹æ•ˆæœæœ‰é™
2. **ç¯å¢ƒä¾èµ–**: æŸäº›åŠŸèƒ½ä¾èµ–ç‰¹å®šçš„ç³»ç»Ÿç¯å¢ƒå’Œæƒé™é…ç½®
3. **æ¨¡å‹è°ƒä¼˜**: å¼‚å¸¸æ£€æµ‹æ¨¡å‹çš„å‚æ•°éœ€è¦æ ¹æ®å®é™…ç¯å¢ƒè¿›è¡Œè°ƒä¼˜

#### æ”¹è¿›æªæ–½

1. **å¢åŠ æ•°æ®ç§¯ç´¯æ—¶é—´**: å»ºè®®è¿è¡Œç³»ç»Ÿä¸€æ®µæ—¶é—´åå†è¿›è¡Œå®Œæ•´æµ‹è¯•
2. **ä¼˜åŒ–ç¯å¢ƒé…ç½®**: ç¡®ä¿ç³»ç»Ÿæœ‰è¶³å¤Ÿçš„æ–‡ä»¶è®¿é—®æƒé™
3. **å‚æ•°è‡ªé€‚åº”**: å®ç°æ ¹æ®æ•°æ®ç‰¹å¾è‡ªåŠ¨è°ƒæ•´æ¨¡å‹å‚æ•°çš„åŠŸèƒ½
"""
        
        report_content += f"""
#### æ€»ä½“è¯„ä»·

å¼‚å¸¸æ¨¡å¼æ£€æµ‹ç³»ç»ŸæˆåŠŸå®ç°äº†æ™ºèƒ½è¿ç»´åŠ©æ‰‹çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œå…·å¤‡äº†ä»æ•°æ®é‡‡é›†åˆ°è‡ªåŠ¨åŒ–æ£€æµ‹çš„å®Œæ•´èƒ½åŠ›ã€‚ç³»ç»Ÿåœ¨æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„ç¨³å®šæ€§å’Œå®ç”¨æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒè¿ç»´å›¢é˜Ÿçš„æ—¥å¸¸å·¥ä½œã€‚

**æµ‹è¯•é€šè¿‡ç‡**: {successful_tests}/{total_tests} ({(successful_tests/total_tests*100):.1f}%)

**æ¨èéƒ¨ç½²**: {'æ˜¯' if successful_tests >= total_tests * 0.8 else 'å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–åéƒ¨ç½²'}
"""
        
        # ä¿å­˜æŠ¥å‘Š
        with open(self.test_output_dir / "test_report.md", 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"\næµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.test_output_dir}/test_report.md")
    
    def run_all_tests(self) -> None:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        self.logger.info("ğŸš€ å¼€å§‹è¿è¡Œå¼‚å¸¸æ¨¡å¼æ•æ‰å’Œæ‰«æç»¼åˆæµ‹è¯•...")
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        tests_to_run = [
            ('data_collection', self.test_1_data_collection_performance),
            ('anomaly_detection', self.test_2_anomaly_detection_accuracy),
            ('pattern_extraction', self.test_3_pattern_extraction_quality),
            ('scanner_generation', self.test_4_scanner_generation_effectiveness),
            ('scanner_execution', self.test_5_scanner_execution_performance),
            ('end_to_end', self.test_6_end_to_end_pipeline)
        ]
        
        for test_name, test_func in tests_to_run:
            try:
                self.logger.info(f"æ­£åœ¨æ‰§è¡Œæµ‹è¯•: {test_name}")
                result = test_func()
                self.test_results['tests'][test_name] = result
                
                if result.get('success'):
                    self.logger.info(f"âœ… æµ‹è¯• {test_name} å®Œæˆ")
                else:
                    self.logger.warning(f"âŒ æµ‹è¯• {test_name} å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    
            except Exception as e:
                self.logger.error(f"âŒ æµ‹è¯• {test_name} æ‰§è¡Œå¼‚å¸¸: {e}")
                self.test_results['tests'][test_name] = {
                    'test_name': test_name,
                    'success': False,
                    'error': str(e),
                    'start_time': datetime.now().isoformat(),
                    'end_time': datetime.now().isoformat()
                }
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_test_report()
        
        self.logger.info("ğŸ‰ ç»¼åˆæµ‹è¯•å®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼‚å¸¸æ¨¡å¼æ•æ‰å’Œæ‰«ææµ‹è¯• - ç»¼åˆæµ‹è¯•å·¥å…·")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = ComprehensiveTestSuite()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_suite.run_all_tests()
    
    print("\nğŸ“Š æµ‹è¯•å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ° test_results/ ç›®å½•")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - test_results.json: å®Œæ•´æµ‹è¯•æ•°æ®")
    print("  - performance_table.txt: æ€§èƒ½æŒ‡æ ‡è¡¨æ ¼")
    print("  - test_report.md: å®Œæ•´æµ‹è¯•æŠ¥å‘Š")
    print("  - performance_comparison.png: æ€§èƒ½å¯¹æ¯”å›¾è¡¨")

if __name__ == "__main__":
    main()
