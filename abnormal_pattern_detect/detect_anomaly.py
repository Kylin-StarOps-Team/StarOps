#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼‚å¸¸æ£€æµ‹æ¨¡å— - ä½¿ç”¨PyODåº“å’ŒIsolation Forestæ£€æµ‹å¼‚å¸¸
"""

import pandas as pd
import numpy as np
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple, Optional
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

try:
    from pyod.models.iforest import IForest
    from pyod.models.lof import LOF
    from pyod.models.knn import KNN
    from sklearn.preprocessing import StandardScaler
    from sklearn.decomposition import PCA
except ImportError as e:
    print(f"è­¦å‘Š: ç¼ºå°‘ä¾èµ–åŒ… {e}")
    print("è¯·è¿è¡Œ: pip install pyod scikit-learn")


class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, output_dir: str = "data", contamination: float = 0.1):
        """
        åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            contamination: å¼‚å¸¸æ¯”ä¾‹ä¼°è®¡ï¼ˆ0.05-0.2ï¼‰
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.contamination = contamination
        
        # è¾“å…¥æ–‡ä»¶
        self.metrics_file = self.output_dir / "metrics.csv"
        self.process_file = self.output_dir / "processes.csv"
        self.parsed_logs_file = self.output_dir / "parsed_logs.json"
        
        # è¾“å‡ºæ–‡ä»¶
        self.anomalies_file = self.output_dir / "anomalies.csv"
        self.anomaly_summary_file = self.output_dir / "anomaly_summary.json"
        
        # æ¨¡å‹é…ç½®
        self.models = {
            'isolation_forest': IForest(contamination=contamination, random_state=42),
            'lof': LOF(contamination=contamination),
            'knn': KNN(contamination=contamination)
        }
        
        # ç‰¹å¾åˆ—å®šä¹‰
        self.system_features = [
            'cpu_percent', 'memory_percent', 'disk_usage_percent', 
            'network_connections', 'process_count'
        ]
        
        self.process_features = [
            'cpu_percent', 'memory_percent', 'memory_rss', 'connections'
        ]
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def load_metrics_data(self, hours_back: int = 24) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """åŠ è½½æŒ‡æ ‡æ•°æ®"""
        try:
            # åŠ è½½ç³»ç»ŸæŒ‡æ ‡
            if self.metrics_file.exists():
                system_df = pd.read_csv(self.metrics_file)
                system_df['timestamp'] = pd.to_datetime(system_df['timestamp'])
                self.logger.info(f"åŠ è½½ç³»ç»ŸæŒ‡æ ‡: {len(system_df)} æ¡è®°å½•")
            else:
                system_df = pd.DataFrame()
                self.logger.warning("ç³»ç»ŸæŒ‡æ ‡æ–‡ä»¶ä¸å­˜åœ¨")
            
            # åŠ è½½è¿›ç¨‹æŒ‡æ ‡
            if self.process_file.exists():
                process_df = pd.read_csv(self.process_file)
                process_df['timestamp'] = pd.to_datetime(process_df['timestamp'])
                self.logger.info(f"åŠ è½½è¿›ç¨‹æŒ‡æ ‡: {len(process_df)} æ¡è®°å½•")
            else:
                process_df = pd.DataFrame()
                self.logger.warning("è¿›ç¨‹æŒ‡æ ‡æ–‡ä»¶ä¸å­˜åœ¨")
            
            # è¿‡æ»¤æ—¶é—´çª—å£
            if hours_back > 0:
                cutoff_time = datetime.now() - timedelta(hours=hours_back)
                if not system_df.empty:
                    system_df = system_df[system_df['timestamp'] > cutoff_time]
                if not process_df.empty:
                    process_df = process_df[process_df['timestamp'] > cutoff_time]
            
            return system_df, process_df
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame(), pd.DataFrame()
    
    def preprocess_system_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, pd.DataFrame]:
        """é¢„å¤„ç†ç³»ç»Ÿæ•°æ®"""
        if df.empty:
            return np.array([]), df
        
        # é€‰æ‹©ç‰¹å¾åˆ—
        feature_df = df[self.system_features].copy()
        
        # å¤„ç†ç¼ºå¤±å€¼
        feature_df = feature_df.fillna(feature_df.mean())
        
        # æ·»åŠ æ´¾ç”Ÿç‰¹å¾
        if len(feature_df) > 1:
            # è®¡ç®—å˜åŒ–ç‡
            feature_df['cpu_change'] = feature_df['cpu_percent'].diff().fillna(0)
            feature_df['memory_change'] = feature_df['memory_percent'].diff().fillna(0)
            
            # è®¡ç®—ç§»åŠ¨å¹³å‡
            window_size = min(5, len(feature_df))
            feature_df['cpu_ma'] = feature_df['cpu_percent'].rolling(window=window_size).mean().fillna(feature_df['cpu_percent'])
            feature_df['memory_ma'] = feature_df['memory_percent'].rolling(window=window_size).mean().fillna(feature_df['memory_percent'])
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(feature_df)
        
        return features_scaled, feature_df
    
    def preprocess_process_data(self, df: pd.DataFrame) -> Tuple[Dict[str, np.ndarray], Dict[str, pd.DataFrame]]:
        """é¢„å¤„ç†è¿›ç¨‹æ•°æ®ï¼ˆæŒ‰æœåŠ¡åˆ†ç»„ï¼‰"""
        if df.empty:
            return {}, {}
        
        features_by_service = {}
        feature_dfs_by_service = {}
        
        # æŒ‰è¿›ç¨‹ååˆ†ç»„
        for process_name in df['name'].unique():
            process_df = df[df['name'] == process_name].copy()
            
            if len(process_df) < 3:  # æ•°æ®ç‚¹å¤ªå°‘ï¼Œè·³è¿‡
                continue
            
            # é€‰æ‹©ç‰¹å¾åˆ—
            feature_cols = [col for col in self.process_features if col in process_df.columns]
            feature_df = process_df[feature_cols].copy()
            
            # å¤„ç†ç¼ºå¤±å€¼
            feature_df = feature_df.fillna(feature_df.mean())
            
            # æ·»åŠ æ´¾ç”Ÿç‰¹å¾
            if len(feature_df) > 1:
                feature_df['cpu_change'] = feature_df['cpu_percent'].diff().fillna(0)
                feature_df['memory_change'] = feature_df['memory_percent'].diff().fillna(0)
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(feature_df)
            
            features_by_service[process_name] = features_scaled
            feature_dfs_by_service[process_name] = feature_df
        
        return features_by_service, feature_dfs_by_service
    
    def detect_system_anomalies(self, system_df: pd.DataFrame) -> Dict[str, Any]:
        """æ£€æµ‹ç³»ç»Ÿçº§å¼‚å¸¸"""
        if system_df.empty:
            return {'anomalies': [], 'summary': {'total': 0, 'by_model': {}}}
        
        features, feature_df = self.preprocess_system_data(system_df)
        
        if features.size == 0:
            return {'anomalies': [], 'summary': {'total': 0, 'by_model': {}}}
        
        anomaly_results = {
            'anomalies': [],
            'summary': {'total': 0, 'by_model': {}}
        }
        
        # ä½¿ç”¨å¤šä¸ªæ¨¡å‹æ£€æµ‹å¼‚å¸¸
        all_anomaly_scores = {}
        
        for model_name, model in self.models.items():
            try:
                # æ£€æŸ¥æ ·æœ¬æ•°é‡æ˜¯å¦è¶³å¤Ÿ
                if features.shape[0] < 2:
                    self.logger.warning(f"æ ·æœ¬æ•°é‡ä¸è¶³ï¼Œè·³è¿‡ {model_name} æ¨¡å‹")
                    continue
                
                # å¯¹äºéœ€è¦é‚»å±…çš„æ¨¡å‹ï¼Œæ£€æŸ¥æ ·æœ¬æ•°é‡
                if model_name in ['lof', 'knn'] and features.shape[0] < 5:
                    self.logger.warning(f"æ ·æœ¬æ•°é‡ä¸è¶³ï¼Œè·³è¿‡ {model_name} æ¨¡å‹")
                    continue
                
                # è®­ç»ƒæ¨¡å‹å¹¶é¢„æµ‹
                anomaly_labels = model.fit_predict(features)
                anomaly_scores = model.decision_function(features)
                
                # æ‰¾å‡ºå¼‚å¸¸ç‚¹
                anomaly_indices = np.where(anomaly_labels == 1)[0]
                
                all_anomaly_scores[model_name] = {
                    'labels': anomaly_labels,
                    'scores': anomaly_scores,
                    'anomaly_count': len(anomaly_indices)
                }
                
                anomaly_results['summary']['by_model'][model_name] = len(anomaly_indices)
                
                self.logger.info(f"{model_name} æ£€æµ‹åˆ° {len(anomaly_indices)} ä¸ªç³»ç»Ÿå¼‚å¸¸")
                
            except Exception as e:
                self.logger.error(f"æ¨¡å‹ {model_name} æ£€æµ‹å¤±è´¥: {e}")
                continue
        
        # é›†æˆå¤šä¸ªæ¨¡å‹çš„ç»“æœï¼ˆæŠ•ç¥¨æœºåˆ¶ï¼‰
        if all_anomaly_scores:
            ensemble_anomalies = self._ensemble_anomaly_detection(all_anomaly_scores, features.shape[0])
            
            # æ„å»ºå¼‚å¸¸è®°å½•
            for idx in ensemble_anomalies:
                original_idx = system_df.index[idx]
                # å°†Timestampè½¬æ¢ä¸ºISOæ ¼å¼å­—ç¬¦ä¸²
                timestamp = system_df.loc[original_idx, 'timestamp']
                if hasattr(timestamp, 'isoformat'):
                    timestamp_str = timestamp.isoformat()
                else:
                    timestamp_str = str(timestamp)
                
                anomaly_record = {
                    'timestamp': timestamp_str,
                    'type': 'system',
                    'anomaly_index': int(idx),
                    'metrics': {
                        'cpu_percent': float(system_df.loc[original_idx, 'cpu_percent']),
                        'memory_percent': float(system_df.loc[original_idx, 'memory_percent']),
                        'disk_usage_percent': float(system_df.loc[original_idx, 'disk_usage_percent']),
                        'network_connections': int(system_df.loc[original_idx, 'network_connections'])
                    },
                    'scores': {name: float(scores['scores'][idx]) for name, scores in all_anomaly_scores.items()},
                    'severity': self._calculate_severity(all_anomaly_scores, idx)
                }
                anomaly_results['anomalies'].append(anomaly_record)
            
            anomaly_results['summary']['total'] = len(ensemble_anomalies)
        
        return anomaly_results
    
    def detect_process_anomalies(self, process_df: pd.DataFrame) -> Dict[str, Any]:
        """æ£€æµ‹è¿›ç¨‹çº§å¼‚å¸¸"""
        if process_df.empty:
            return {'anomalies': [], 'summary': {'total': 0, 'by_service': {}}}
        
        features_by_service, feature_dfs_by_service = self.preprocess_process_data(process_df)
        
        anomaly_results = {
            'anomalies': [],
            'summary': {'total': 0, 'by_service': {}}
        }
        
        for service_name, features in features_by_service.items():
            service_df = process_df[process_df['name'] == service_name].copy()
            
            try:
                # æ£€æŸ¥æ ·æœ¬æ•°é‡æ˜¯å¦è¶³å¤Ÿ
                if features.shape[0] < 2:
                    self.logger.warning(f"æœåŠ¡ {service_name} æ ·æœ¬æ•°é‡ä¸è¶³ï¼Œè·³è¿‡å¼‚å¸¸æ£€æµ‹")
                    continue
                
                # ä½¿ç”¨Isolation Forestæ£€æµ‹è¿›ç¨‹å¼‚å¸¸
                model = IForest(contamination=self.contamination, random_state=42)
                anomaly_labels = model.fit_predict(features)
                anomaly_scores = model.decision_function(features)
                
                anomaly_indices = np.where(anomaly_labels == 1)[0]
                
                anomaly_results['summary']['by_service'][service_name] = len(anomaly_indices)
                
                # æ„å»ºå¼‚å¸¸è®°å½•
                for idx in anomaly_indices:
                    original_idx = service_df.index[idx]
                    # å°†Timestampè½¬æ¢ä¸ºISOæ ¼å¼å­—ç¬¦ä¸²
                    timestamp = service_df.loc[original_idx, 'timestamp']
                    if hasattr(timestamp, 'isoformat'):
                        timestamp_str = timestamp.isoformat()
                    else:
                        timestamp_str = str(timestamp)
                    
                    anomaly_record = {
                        'timestamp': timestamp_str,
                        'type': 'process',
                        'service': service_name,
                        'pid': int(service_df.loc[original_idx, 'pid']),
                        'anomaly_index': int(idx),
                        'metrics': {
                            'cpu_percent': float(service_df.loc[original_idx, 'cpu_percent']),
                            'memory_percent': float(service_df.loc[original_idx, 'memory_percent']),
                            'memory_rss': int(service_df.loc[original_idx, 'memory_rss']),
                            'connections': int(service_df.loc[original_idx, 'connections'])
                        },
                        'anomaly_score': float(anomaly_scores[idx]),
                        'severity': 'high' if anomaly_scores[idx] < -0.5 else 'medium'
                    }
                    anomaly_results['anomalies'].append(anomaly_record)
                
                self.logger.info(f"{service_name} æ£€æµ‹åˆ° {len(anomaly_indices)} ä¸ªè¿›ç¨‹å¼‚å¸¸")
                
            except Exception as e:
                self.logger.error(f"æ£€æµ‹ {service_name} è¿›ç¨‹å¼‚å¸¸å¤±è´¥: {e}")
                continue
        
        anomaly_results['summary']['total'] = len(anomaly_results['anomalies'])
        return anomaly_results
    
    def integrate_log_anomalies(self) -> Dict[str, Any]:
        """æ•´åˆæ—¥å¿—å¼‚å¸¸ä¿¡æ¯"""
        log_anomalies = {
            'anomalies': [],
            'summary': {'total': 0, 'by_service': {}}
        }
        
        try:
            if not self.parsed_logs_file.exists():
                return log_anomalies
            
            with open(self.parsed_logs_file, 'r', encoding='utf-8') as f:
                log_data = json.load(f)
            
            # åˆ†ææ¯ä¸ªæœåŠ¡çš„æ—¥å¿—å¼‚å¸¸
            for service, service_data in log_data.get('services', {}).items():
                error_count = 0
                critical_count = 0
                
                for file_data in service_data.get('files', []):
                    level_counts = file_data.get('summary', {}).get('by_level', {})
                    error_count += level_counts.get('error', 0)
                    critical_count += level_counts.get('critical', 0)
                
                total_anomalies = error_count + critical_count
                
                if total_anomalies > 0:
                    log_anomaly = {
                        'timestamp': log_data.get('parse_time'),
                        'type': 'log',
                        'service': service,
                        'metrics': {
                            'error_count': error_count,
                            'critical_count': critical_count,
                            'total_anomalies': total_anomalies
                        },
                        'severity': 'critical' if critical_count > 0 else 'high' if error_count > 10 else 'medium'
                    }
                    log_anomalies['anomalies'].append(log_anomaly)
                
                log_anomalies['summary']['by_service'][service] = total_anomalies
            
            log_anomalies['summary']['total'] = len(log_anomalies['anomalies'])
            
        except Exception as e:
            self.logger.error(f"æ•´åˆæ—¥å¿—å¼‚å¸¸å¤±è´¥: {e}")
        
        return log_anomalies
    
    def _ensemble_anomaly_detection(self, model_results: Dict, total_samples: int) -> List[int]:
        """é›†æˆå¤šä¸ªæ¨¡å‹çš„å¼‚å¸¸æ£€æµ‹ç»“æœ"""
        # æŠ•ç¥¨æœºåˆ¶ï¼šè‡³å°‘ä¸¤ä¸ªæ¨¡å‹è®¤ä¸ºæ˜¯å¼‚å¸¸çš„ç‚¹
        anomaly_votes = np.zeros(total_samples)
        
        for model_name, results in model_results.items():
            anomaly_indices = np.where(results['labels'] == 1)[0]
            anomaly_votes[anomaly_indices] += 1
        
        # è‡³å°‘2ä¸ªæ¨¡å‹æŠ•ç¥¨ä¸ºå¼‚å¸¸
        ensemble_anomalies = np.where(anomaly_votes >= 2)[0]
        
        return ensemble_anomalies.tolist()
    
    def _calculate_severity(self, model_results: Dict, anomaly_idx: int) -> str:
        """è®¡ç®—å¼‚å¸¸ä¸¥é‡ç¨‹åº¦"""
        scores = [results['scores'][anomaly_idx] for results in model_results.values()]
        avg_score = np.mean(scores)
        
        if avg_score < -0.6:
            return 'critical'
        elif avg_score < -0.3:
            return 'high'
        elif avg_score < 0:
            return 'medium'
        else:
            return 'low'
    
    def run_anomaly_detection(self, hours_back: int = 24) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„å¼‚å¸¸æ£€æµ‹æµç¨‹"""
        self.logger.info("å¼€å§‹å¼‚å¸¸æ£€æµ‹...")
        
        # åŠ è½½æ•°æ®
        system_df, process_df = self.load_metrics_data(hours_back)
        
        # æ£€æµ‹ç³»ç»Ÿå¼‚å¸¸
        system_anomalies = self.detect_system_anomalies(system_df)
        
        # æ£€æµ‹è¿›ç¨‹å¼‚å¸¸
        process_anomalies = self.detect_process_anomalies(process_df)
        
        # æ•´åˆæ—¥å¿—å¼‚å¸¸
        log_anomalies = self.integrate_log_anomalies()
        
        # åˆå¹¶æ‰€æœ‰å¼‚å¸¸
        all_anomalies = {
            'detection_time': datetime.now().isoformat(),
            'time_window_hours': hours_back,
            'system_anomalies': system_anomalies,
            'process_anomalies': process_anomalies,
            'log_anomalies': log_anomalies,
            'summary': {
                'total_anomalies': (
                    system_anomalies['summary']['total'] + 
                    process_anomalies['summary']['total'] + 
                    log_anomalies['summary']['total']
                ),
                'by_type': {
                    'system': system_anomalies['summary']['total'],
                    'process': process_anomalies['summary']['total'],
                    'log': log_anomalies['summary']['total']
                }
            }
        }
        
        return all_anomalies
    
    def save_anomaly_results(self, anomaly_results: Dict[str, Any]):
        """ä¿å­˜å¼‚å¸¸æ£€æµ‹ç»“æœ"""
        try:
            print("=======================")
            print(anomaly_results)
            # ä¿å­˜è¯¦ç»†ç»“æœ
            with open(self.anomaly_summary_file, 'w', encoding='utf-8') as f:
                json.dump(anomaly_results, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜å¼‚å¸¸åˆ—è¡¨åˆ°CSV
            all_anomalies = []
            
            # ç³»ç»Ÿå¼‚å¸¸
            for anomaly in anomaly_results['system_anomalies']['anomalies']:
                all_anomalies.append({
                    'timestamp': anomaly['timestamp'],
                    'type': 'system',
                    'service': 'system',
                    'severity': anomaly['severity'],
                    'cpu_percent': anomaly['metrics']['cpu_percent'],
                    'memory_percent': anomaly['metrics']['memory_percent'],
                    'description': 'System resource anomaly detected'
                })
            
            # è¿›ç¨‹å¼‚å¸¸
            for anomaly in anomaly_results['process_anomalies']['anomalies']:
                all_anomalies.append({
                    'timestamp': anomaly['timestamp'],
                    'type': 'process',
                    'service': anomaly['service'],
                    'severity': anomaly['severity'],
                    'cpu_percent': anomaly['metrics']['cpu_percent'],
                    'memory_percent': anomaly['metrics']['memory_percent'],
                    'description': f'Process anomaly detected for {anomaly["service"]}'
                })
            
            # æ—¥å¿—å¼‚å¸¸
            for anomaly in anomaly_results['log_anomalies']['anomalies']:
                all_anomalies.append({
                    'timestamp': anomaly['timestamp'],
                    'type': 'log',
                    'service': anomaly['service'],
                    'severity': anomaly['severity'],
                    'cpu_percent': 0,
                    'memory_percent': 0,
                    'description': f'Log anomaly: {anomaly["metrics"]["error_count"]} errors, {anomaly["metrics"]["critical_count"]} critical'
                })
            
            # ä¿å­˜åˆ°CSV
            if all_anomalies:
                anomalies_df = pd.DataFrame(all_anomalies)
                anomalies_df.to_csv(self.anomalies_file, index=False)
            
            self.logger.info(f"å¼‚å¸¸æ£€æµ‹ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¼‚å¸¸æ£€æµ‹ç»“æœå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°æ¼”ç¤º"""
    detector = AnomalyDetector(output_dir="data", contamination=0.1)
    
    print("ğŸ” å¼€å§‹å¼‚å¸¸æ£€æµ‹...")
    
    try:
        # è¿è¡Œå¼‚å¸¸æ£€æµ‹
        results = detector.run_anomaly_detection(hours_back=24)
        
        # ä¿å­˜ç»“æœ
        detector.save_anomaly_results(results)
        
        # æ˜¾ç¤ºæ‘˜è¦
        summary = results['summary']
        print(f"\nğŸ“Š å¼‚å¸¸æ£€æµ‹æ‘˜è¦:")
        print(f"  - æ€»å¼‚å¸¸æ•°: {summary['total_anomalies']}")
        print(f"  - ç³»ç»Ÿå¼‚å¸¸: {summary['by_type']['system']}")
        print(f"  - è¿›ç¨‹å¼‚å¸¸: {summary['by_type']['process']}")
        print(f"  - æ—¥å¿—å¼‚å¸¸: {summary['by_type']['log']}")
        
        # æ˜¾ç¤ºç³»ç»Ÿå¼‚å¸¸è¯¦æƒ…
        if results['system_anomalies']['anomalies']:
            print(f"\nâš ï¸ ç³»ç»Ÿå¼‚å¸¸:")
            for anomaly in results['system_anomalies']['anomalies'][:3]:
                print(f"  - {anomaly['timestamp'][:19]} ({anomaly['severity']}): "
                      f"CPU {anomaly['metrics']['cpu_percent']:.1f}%, "
                      f"MEM {anomaly['metrics']['memory_percent']:.1f}%")
        
        # æ˜¾ç¤ºè¿›ç¨‹å¼‚å¸¸è¯¦æƒ…
        if results['process_anomalies']['anomalies']:
            print(f"\nğŸ”§ è¿›ç¨‹å¼‚å¸¸:")
            for anomaly in results['process_anomalies']['anomalies'][:3]:
                print(f"  - {anomaly['service']} ({anomaly['timestamp'][:19]}): "
                      f"CPU {anomaly['metrics']['cpu_percent']:.1f}%, "
                      f"MEM {anomaly['metrics']['memory_percent']:.1f}%")
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {detector.output_dir}")
        
    except Exception as e:
        print(f"âŒ å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")


if __name__ == "__main__":
    main() 