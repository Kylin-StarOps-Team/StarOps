#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼‚å¸¸æ¨¡å¼æå–æ¨¡å— - ä»å¼‚å¸¸æ•°æ®ä¸­å½’çº³æ•…éšœç‰¹å¾ç»„åˆ
"""

import pandas as pd
import numpy as np
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
from collections import defaultdict, Counter
import re


class PatternExtractor:
    """å¼‚å¸¸æ¨¡å¼æå–å™¨"""
    
    def __init__(self, output_dir: str = "data"):
        """åˆå§‹åŒ–æ¨¡å¼æå–å™¨"""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # è¾“å…¥æ–‡ä»¶
        self.anomalies_file = self.output_dir / "anomalies.csv"
        self.anomaly_summary_file = self.output_dir / "anomaly_summary.json"
        self.parsed_logs_file = self.output_dir / "parsed_logs.json"
        
        # è¾“å‡ºæ–‡ä»¶
        self.patterns_file = self.output_dir / "extracted_patterns.json"
        
        # æ¨¡å¼æå–é…ç½®
        self.metric_thresholds = {
            'cpu_percent': {'high': 80, 'critical': 90},
            'memory_percent': {'high': 75, 'critical': 85},
            'disk_usage_percent': {'high': 80, 'critical': 90},
            'network_connections': {'high': 1000, 'critical': 2000}
        }
        
        # å…³é”®è¯æƒé‡
        self.keyword_weights = {
            'critical': ['fatal', 'critical', 'emergency', 'panic', 'corrupt', 'crash'],
            'error': ['error', 'failed', 'failure', 'timeout', '502', '503', '504', '500'],
            'warning': ['warning', 'slow', 'retry', 'deprecated']
        }
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def load_anomaly_data(self) -> Dict[str, Any]:
        """åŠ è½½å¼‚å¸¸æ£€æµ‹æ•°æ®"""
        anomaly_data = {}
        
        try:
            # åŠ è½½å¼‚å¸¸æ‘˜è¦
            if self.anomaly_summary_file.exists():
                with open(self.anomaly_summary_file, 'r', encoding='utf-8') as f:
                    anomaly_data = json.load(f)
                self.logger.info("å¼‚å¸¸æ•°æ®åŠ è½½æˆåŠŸ")
            else:
                self.logger.warning("å¼‚å¸¸æ‘˜è¦æ–‡ä»¶ä¸å­˜åœ¨")
                
        except Exception as e:
            self.logger.error(f"åŠ è½½å¼‚å¸¸æ•°æ®å¤±è´¥: {e}")
        
        return anomaly_data
    
    def extract_metric_patterns(self, anomalies: List[Dict]) -> List[Dict[str, Any]]:
        """æå–æŒ‡æ ‡å¼‚å¸¸æ¨¡å¼"""
        patterns = []
        
        if not anomalies:
            return patterns
        
        # æŒ‰æœåŠ¡åˆ†ç»„
        by_service = defaultdict(list)
        for anomaly in anomalies:
            service = anomaly.get('service', 'system')
            by_service[service].append(anomaly)
        
        # ä¸ºæ¯ä¸ªæœåŠ¡æå–æ¨¡å¼
        for service, service_anomalies in by_service.items():
            if len(service_anomalies) < 2:  # è‡³å°‘éœ€è¦2ä¸ªå¼‚å¸¸æ ·æœ¬
                continue
            
            # æå–æŒ‡æ ‡ç‰¹å¾
            metrics_data = []
            for anomaly in service_anomalies:
                metrics = anomaly.get('metrics', {})
                if metrics:
                    metrics_data.append(metrics)
            
            if not metrics_data:
                continue
            
            # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
            pattern = self._calculate_metric_statistics(service, metrics_data)
            
            # æ·»åŠ æ—¶é—´ç‰¹å¾
            timestamps = [anomaly.get('timestamp') for anomaly in service_anomalies if anomaly.get('timestamp')]
            pattern['temporal_features'] = self._analyze_temporal_patterns(timestamps)
            
            # æ·»åŠ ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
            severities = [anomaly.get('severity', 'medium') for anomaly in service_anomalies]
            pattern['severity_distribution'] = dict(Counter(severities))
            
            # è®¾ç½®æ¨¡å¼å…ƒæ•°æ®
            pattern.update({
                'pattern_id': f"{service}_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                'pattern_type': 'metric_anomaly',
                'service': service,
                'sample_count': len(service_anomalies),
                'extraction_time': datetime.now().isoformat()
            })
            
            patterns.append(pattern)
        
        return patterns
    
    def extract_log_patterns(self, log_anomalies: List[Dict]) -> List[Dict[str, Any]]:
        """æå–æ—¥å¿—å¼‚å¸¸æ¨¡å¼"""
        patterns = []
        
        try:
            # åŠ è½½è¯¦ç»†æ—¥å¿—æ•°æ®
            if not self.parsed_logs_file.exists():
                return patterns
            
            with open(self.parsed_logs_file, 'r', encoding='utf-8') as f:
                log_data = json.load(f)
            
            # æŒ‰æœåŠ¡åˆ†ææ—¥å¿—æ¨¡å¼
            for service, service_data in log_data.get('services', {}).items():
                if service not in [anomaly.get('service') for anomaly in log_anomalies]:
                    continue  # è·³è¿‡æ²¡æœ‰å¼‚å¸¸çš„æœåŠ¡
                
                # æ”¶é›†é”™è¯¯æ—¥å¿—
                error_messages = []
                error_keywords = Counter()
                
                for file_data in service_data.get('files', []):
                    for entry in file_data.get('log_entries', []):
                        if entry.get('level') in ['error', 'critical']:
                            error_messages.append(entry.get('message', ''))
                            
                            # æå–å…³é”®è¯
                            keywords = self._extract_keywords_from_message(entry.get('message', ''))
                            error_keywords.update(keywords)
                
                if error_messages:
                    pattern = self._analyze_log_patterns(service, error_messages, error_keywords)
                    
                    # æŸ¥æ‰¾å¯¹åº”çš„æ—¥å¿—å¼‚å¸¸
                    service_log_anomaly = next(
                        (anomaly for anomaly in log_anomalies if anomaly.get('service') == service), 
                        {}
                    )
                    
                    pattern.update({
                        'pattern_id': f"{service}_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        'pattern_type': 'log_anomaly',
                        'service': service,
                        'error_count': len(error_messages),
                        'severity': service_log_anomaly.get('severity', 'medium'),
                        'extraction_time': datetime.now().isoformat()
                    })
                    
                    patterns.append(pattern)
        
        except Exception as e:
            self.logger.error(f"æå–æ—¥å¿—æ¨¡å¼å¤±è´¥: {e}")
        
        return patterns
    
    def extract_composite_patterns(self, metric_patterns: List[Dict], log_patterns: List[Dict]) -> List[Dict[str, Any]]:
        """æå–å¤åˆæ¨¡å¼ï¼ˆæŒ‡æ ‡+æ—¥å¿—ï¼‰"""
        composite_patterns = []
        
        # æŒ‰æœåŠ¡åŒ¹é…æŒ‡æ ‡å’Œæ—¥å¿—æ¨¡å¼
        metric_by_service = {p['service']: p for p in metric_patterns}
        log_by_service = {p['service']: p for p in log_patterns}
        
        common_services = set(metric_by_service.keys()) & set(log_by_service.keys())
        
        for service in common_services:
            metric_pattern = metric_by_service[service]
            log_pattern = log_by_service[service]
            
            # åˆ›å»ºå¤åˆæ¨¡å¼
            composite_pattern = {
                'pattern_id': f"{service}_composite_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                'pattern_name': f"{service}ç»¼åˆå¼‚å¸¸æ¨¡å¼",
                'pattern_type': 'composite_anomaly',
                'service': service,
                'extraction_time': datetime.now().isoformat(),
                
                # æŒ‡æ ‡ç‰¹å¾
                'metrics': {
                    'cpu': metric_pattern.get('cpu_statistics', {}),
                    'memory': metric_pattern.get('memory_statistics', {}),
                    'thresholds': self._determine_thresholds(metric_pattern)
                },
                
                # æ—¥å¿—ç‰¹å¾
                'logs': {
                    'keywords': log_pattern.get('top_keywords', []),
                    'patterns': log_pattern.get('common_patterns', []),
                    'frequency': log_pattern.get('error_frequency', 0),
                    'error_rate': log_pattern.get('error_rate', 0)
                },
                
                # å¤åˆæ¡ä»¶
                'conditions': self._generate_composite_conditions(metric_pattern, log_pattern),
                
                # ä¸¥é‡ç¨‹åº¦
                'severity': self._determine_composite_severity(metric_pattern, log_pattern),
                
                # ç½®ä¿¡åº¦
                'confidence': self._calculate_pattern_confidence(metric_pattern, log_pattern)
            }
            
            composite_patterns.append(composite_pattern)
        
        return composite_patterns
    
    def _calculate_metric_statistics(self, service: str, metrics_data: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—æŒ‡æ ‡ç»Ÿè®¡ç‰¹å¾"""
        if not metrics_data:
            return {}
        
        df = pd.DataFrame(metrics_data)
        statistics = {}
        
        for metric in ['cpu_percent', 'memory_percent', 'disk_usage_percent', 'network_connections']:
            if metric in df.columns:
                values = df[metric].dropna()
                if len(values) > 0:
                    statistics[f'{metric.replace("_percent", "")}_statistics'] = {
                        'mean': float(np.mean(values)),
                        'std': float(np.std(values)),
                        'min': float(np.min(values)),
                        'max': float(np.max(values)),
                        'median': float(np.median(values)),
                        'samples': len(values)
                    }
        
        return statistics
    
    def _analyze_temporal_patterns(self, timestamps: List[str]) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´æ¨¡å¼"""
        if not timestamps:
            return {}
        
        # è§£ææ—¶é—´æˆ³
        parsed_times = []
        for ts in timestamps:
            if ts:
                try:
                    parsed_times.append(pd.to_datetime(ts))
                except:
                    continue
        
        if len(parsed_times) < 2:
            return {}
        
        # è®¡ç®—æ—¶é—´é—´éš”
        intervals = [(parsed_times[i+1] - parsed_times[i]).total_seconds() 
                    for i in range(len(parsed_times)-1)]
        
        # åˆ†æå°æ—¶åˆ†å¸ƒ
        hours = [t.hour for t in parsed_times]
        hour_distribution = dict(Counter(hours))
        
        return {
            'time_span_hours': (max(parsed_times) - min(parsed_times)).total_seconds() / 3600,
            'avg_interval_minutes': np.mean(intervals) / 60 if intervals else 0,
            'hour_distribution': hour_distribution,
            'peak_hours': [h for h, count in Counter(hours).most_common(3)]
        }
    
    def _extract_keywords_from_message(self, message: str) -> List[str]:
        """ä»æ—¥å¿—æ¶ˆæ¯ä¸­æå–å…³é”®è¯"""
        if not message:
            return []
        
        # è½¬æ¢ä¸ºå°å†™å¹¶æå–å•è¯
        words = re.findall(r'\b\w{3,}\b', message.lower())
        
        # è¿‡æ»¤å¸¸è§è¯å’Œæå–å…³é”®è¯
        keywords = []
        for category, category_keywords in self.keyword_weights.items():
            for keyword in category_keywords:
                if keyword in message.lower():
                    keywords.append(keyword)
        
        # æ·»åŠ å…¶ä»–é‡è¦è¯ï¼ˆæ•°å­—ã€çŠ¶æ€ç ç­‰ï¼‰
        numbers = re.findall(r'\b\d{3,}\b', message)
        keywords.extend(numbers)
        
        return list(set(keywords))  # å»é‡
    
    def _analyze_log_patterns(self, service: str, error_messages: List[str], 
                            error_keywords: Counter) -> Dict[str, Any]:
        """åˆ†ææ—¥å¿—æ¨¡å¼"""
        if not error_messages:
            return {}
        
        # æå–å¸¸è§æ¨¡å¼
        common_patterns = []
        
        # æŸ¥æ‰¾é‡å¤çš„é”™è¯¯ç‰‡æ®µ
        message_parts = defaultdict(int)
        for msg in error_messages:
            # æå–å…³é”®é”™è¯¯ç‰‡æ®µï¼ˆå»æ‰å˜é‡éƒ¨åˆ†ï¼‰
            pattern = re.sub(r'\d+', 'NUM', msg)  # æ›¿æ¢æ•°å­—
            pattern = re.sub(r'\b\w+\.\w+\.\w+\.\w+\b', 'IP', pattern)  # æ›¿æ¢IP
            pattern = re.sub(r'/\w+/', '/PATH/', pattern)  # æ›¿æ¢è·¯å¾„
            
            if len(pattern) > 20:  # åªä¿ç•™è¾ƒé•¿çš„æ¨¡å¼
                message_parts[pattern] += 1
        
        # æ‰¾å‡ºå‡ºç°é¢‘ç‡é«˜çš„æ¨¡å¼
        for pattern, count in message_parts.items():
            if count >= max(2, len(error_messages) * 0.1):  # è‡³å°‘å‡ºç°2æ¬¡æˆ–10%
                common_patterns.append({
                    'pattern': pattern[:100],  # é™åˆ¶é•¿åº¦
                    'frequency': count,
                    'percentage': count / len(error_messages)
                })
        
        # è®¡ç®—é”™è¯¯é¢‘ç‡
        total_messages = len(error_messages)
        error_frequency = total_messages
        
        return {
            'total_errors': total_messages,
            'error_frequency': error_frequency,
            'top_keywords': [{'keyword': kw, 'count': count} 
                           for kw, count in error_keywords.most_common(10)],
            'common_patterns': common_patterns[:5],  # æœ€å¤š5ä¸ªå¸¸è§æ¨¡å¼
            'error_rate': min(1.0, total_messages / 100)  # ç®€åŒ–çš„é”™è¯¯ç‡è®¡ç®—
        }
    
    def _determine_thresholds(self, metric_pattern: Dict) -> Dict[str, float]:
        """ç¡®å®šæŒ‡æ ‡é˜ˆå€¼"""
        thresholds = {}
        
        for metric_key, stats in metric_pattern.items():
            if metric_key.endswith('_statistics') and isinstance(stats, dict):
                metric_name = metric_key.replace('_statistics', '')
                
                # åŸºäºç»Ÿè®¡æ•°æ®è®¾ç½®é˜ˆå€¼
                mean = stats.get('mean', 0)
                std = stats.get('std', 0)
                
                # è®¾ç½®ä¸ºå‡å€¼ - 1ä¸ªæ ‡å‡†å·®ä½œä¸ºå¼‚å¸¸é˜ˆå€¼
                threshold = max(0, mean - std)
                
                # åº”ç”¨é»˜è®¤é˜ˆå€¼é™åˆ¶
                if metric_name in ['cpu', 'memory']:
                    threshold = max(threshold, self.metric_thresholds.get(f'{metric_name}_percent', {}).get('high', 80))
                
                thresholds[metric_name] = threshold
        
        return thresholds
    
    def _generate_composite_conditions(self, metric_pattern: Dict, log_pattern: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆå¤åˆæ¡ä»¶"""
        conditions = {
            'logical_operator': 'and',
            'rules': []
        }
        
        # æ·»åŠ æŒ‡æ ‡æ¡ä»¶
        thresholds = self._determine_thresholds(metric_pattern)
        for metric, threshold in thresholds.items():
            conditions['rules'].append({
                'metric': f'{metric}_percent',
                'operator': '>',
                'value': threshold,
                'weight': 0.6
            })
        
        # æ·»åŠ æ—¥å¿—æ¡ä»¶
        top_keywords = log_pattern.get('top_keywords', [])
        if top_keywords:
            main_keyword = top_keywords[0]['keyword']
            conditions['rules'].append({
                'metric': 'log_keywords',
                'operator': 'contains',
                'value': main_keyword,
                'weight': 0.4
            })
        
        return conditions
    
    def _determine_composite_severity(self, metric_pattern: Dict, log_pattern: Dict) -> str:
        """ç¡®å®šå¤åˆæ¨¡å¼ä¸¥é‡ç¨‹åº¦"""
        # åŸºäºæŒ‡æ ‡ä¸¥é‡ç¨‹åº¦
        metric_severity = 'medium'
        for stats_key, stats in metric_pattern.items():
            if stats_key.endswith('_statistics') and isinstance(stats, dict):
                max_value = stats.get('max', 0)
                if max_value > 90:
                    metric_severity = 'critical'
                elif max_value > 80:
                    metric_severity = 'high'
        
        # åŸºäºæ—¥å¿—ä¸¥é‡ç¨‹åº¦
        log_severity = 'medium'
        error_rate = log_pattern.get('error_rate', 0)
        if error_rate > 0.5:
            log_severity = 'critical'
        elif error_rate > 0.2:
            log_severity = 'high'
        
        # å–æœ€é«˜ä¸¥é‡ç¨‹åº¦
        severity_levels = {'low': 1, 'medium': 2, 'high': 3, 'critical': 4}
        max_severity_level = max(
            severity_levels.get(metric_severity, 2),
            severity_levels.get(log_severity, 2)
        )
        
        for severity, level in severity_levels.items():
            if level == max_severity_level:
                return severity
        
        return 'medium'
    
    def _calculate_pattern_confidence(self, metric_pattern: Dict, log_pattern: Dict) -> float:
        """è®¡ç®—æ¨¡å¼ç½®ä¿¡åº¦"""
        confidence_factors = []
        
        # åŸºäºæ ·æœ¬æ•°é‡
        metric_samples = metric_pattern.get('sample_count', 0)
        if metric_samples >= 10:
            confidence_factors.append(0.9)
        elif metric_samples >= 5:
            confidence_factors.append(0.7)
        else:
            confidence_factors.append(0.5)
        
        # åŸºäºæ—¥å¿—é”™è¯¯æ•°é‡
        log_errors = log_pattern.get('total_errors', 0)
        if log_errors >= 20:
            confidence_factors.append(0.9)
        elif log_errors >= 10:
            confidence_factors.append(0.7)
        else:
            confidence_factors.append(0.5)
        
        # åŸºäºæ¨¡å¼ä¸€è‡´æ€§
        common_patterns = log_pattern.get('common_patterns', [])
        if common_patterns and max(p.get('percentage', 0) for p in common_patterns) > 0.5:
            confidence_factors.append(0.8)
        else:
            confidence_factors.append(0.6)
        
        return min(0.95, np.mean(confidence_factors))
    
    def extract_all_patterns(self) -> Dict[str, Any]:
        """æå–æ‰€æœ‰å¼‚å¸¸æ¨¡å¼"""
        self.logger.info("å¼€å§‹æå–å¼‚å¸¸æ¨¡å¼...")
        
        # åŠ è½½å¼‚å¸¸æ•°æ®
        anomaly_data = self.load_anomaly_data()
        
        if not anomaly_data:
            self.logger.warning("æ²¡æœ‰å¼‚å¸¸æ•°æ®å¯ç”¨äºæ¨¡å¼æå–")
            return {'patterns': [], 'summary': {}}
        
        # æå–æŒ‡æ ‡æ¨¡å¼
        system_anomalies = anomaly_data.get('system_anomalies', {}).get('anomalies', [])
        process_anomalies = anomaly_data.get('process_anomalies', {}).get('anomalies', [])
        all_metric_anomalies = system_anomalies + process_anomalies
        
        metric_patterns = self.extract_metric_patterns(all_metric_anomalies)
        
        # æå–æ—¥å¿—æ¨¡å¼
        log_anomalies = anomaly_data.get('log_anomalies', {}).get('anomalies', [])
        log_patterns = self.extract_log_patterns(log_anomalies)
        
        # æå–å¤åˆæ¨¡å¼
        composite_patterns = self.extract_composite_patterns(metric_patterns, log_patterns)
        
        # åˆå¹¶æ‰€æœ‰æ¨¡å¼
        all_patterns = {
            'extraction_time': datetime.now().isoformat(),
            'metric_patterns': metric_patterns,
            'log_patterns': log_patterns,
            'composite_patterns': composite_patterns,
            'summary': {
                'total_patterns': len(metric_patterns) + len(log_patterns) + len(composite_patterns),
                'metric_patterns_count': len(metric_patterns),
                'log_patterns_count': len(log_patterns),
                'composite_patterns_count': len(composite_patterns),
                'services_analyzed': list(set(
                    [p.get('service') for p in metric_patterns + log_patterns + composite_patterns]
                ))
            }
        }
        
        return all_patterns
    
    def save_patterns(self, patterns: Dict[str, Any]):
        """ä¿å­˜æå–çš„æ¨¡å¼ï¼ˆæ”¯æŒè¿½åŠ æ¨¡å¼ï¼‰"""
        try:
            # åŠ è½½ç°æœ‰æ¨¡å¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            existing_patterns = {}
            if self.patterns_file.exists():
                try:
                    with open(self.patterns_file, 'r', encoding='utf-8') as f:
                        existing_patterns = json.load(f)
                    self.logger.info("åŠ è½½ç°æœ‰æ¨¡å¼æ–‡ä»¶")
                except Exception as e:
                    self.logger.warning(f"åŠ è½½ç°æœ‰æ¨¡å¼æ–‡ä»¶å¤±è´¥: {e}")
            
            # åˆå¹¶æ¨¡å¼
            merged_patterns = self._merge_patterns(existing_patterns, patterns)
            
            # ä¿å­˜åˆå¹¶åçš„æ¨¡å¼
            with open(self.patterns_file, 'w', encoding='utf-8') as f:
                json.dump(merged_patterns, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"å¼‚å¸¸æ¨¡å¼å·²ä¿å­˜åˆ°: {self.patterns_file}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ¨¡å¼å¤±è´¥: {e}")
    
    def _merge_patterns(self, existing_patterns: Dict[str, Any], new_patterns: Dict[str, Any]) -> Dict[str, Any]:
        """åˆå¹¶ç°æœ‰æ¨¡å¼å’Œæ–°æ¨¡å¼"""
        merged = {
            'extraction_time': new_patterns.get('extraction_time', datetime.now().isoformat()),
            'metric_patterns': [],
            'log_patterns': [],
            'composite_patterns': [],
            'summary': {}
        }
        
        # åˆå¹¶æŒ‡æ ‡æ¨¡å¼
        existing_metric = existing_patterns.get('metric_patterns', [])
        new_metric = new_patterns.get('metric_patterns', [])
        merged['metric_patterns'] = existing_metric + new_metric
        
        # åˆå¹¶æ—¥å¿—æ¨¡å¼
        existing_log = existing_patterns.get('log_patterns', [])
        new_log = new_patterns.get('log_patterns', [])
        merged['log_patterns'] = existing_log + new_log
        
        # åˆå¹¶å¤åˆæ¨¡å¼
        existing_composite = existing_patterns.get('composite_patterns', [])
        new_composite = new_patterns.get('composite_patterns', [])
        merged['composite_patterns'] = existing_composite + new_composite
        
        # æ›´æ–°æ‘˜è¦
        merged['summary'] = {
            'total_patterns': len(merged['metric_patterns']) + len(merged['log_patterns']) + len(merged['composite_patterns']),
            'metric_patterns_count': len(merged['metric_patterns']),
            'log_patterns_count': len(merged['log_patterns']),
            'composite_patterns_count': len(merged['composite_patterns']),
            'services_analyzed': list(set(
                [p.get('service') for p in merged['metric_patterns'] + merged['log_patterns'] + merged['composite_patterns']]
            ))
        }
        
        self.logger.info(f"æ¨¡å¼åˆå¹¶å®Œæˆ: ç°æœ‰{len(existing_metric + existing_log + existing_composite)}ä¸ªï¼Œæ–°å¢{len(new_metric + new_log + new_composite)}ä¸ª")
        
        return merged


def main():
    """ä¸»å‡½æ•°æ¼”ç¤º"""
    extractor = PatternExtractor(output_dir="data")
    
    print("ğŸ” å¼€å§‹æå–å¼‚å¸¸æ¨¡å¼...")
    
    try:
        # æå–æ‰€æœ‰æ¨¡å¼
        patterns = extractor.extract_all_patterns()
        
        # ä¿å­˜æ¨¡å¼
        extractor.save_patterns(patterns)
        
        # æ˜¾ç¤ºæ‘˜è¦
        summary = patterns['summary']
        print(f"\nğŸ“Š æ¨¡å¼æå–æ‘˜è¦:")
        print(f"  - æ€»æ¨¡å¼æ•°: {summary['total_patterns']}")
        print(f"  - æŒ‡æ ‡æ¨¡å¼: {summary['metric_patterns_count']}")
        print(f"  - æ—¥å¿—æ¨¡å¼: {summary['log_patterns_count']}")
        print(f"  - å¤åˆæ¨¡å¼: {summary['composite_patterns_count']}")
        print(f"  - åˆ†ææœåŠ¡: {', '.join(summary['services_analyzed'])}")
        
        # æ˜¾ç¤ºå¤åˆæ¨¡å¼è¯¦æƒ…
        if patterns['composite_patterns']:
            print(f"\nğŸ”— å¤åˆæ¨¡å¼è¯¦æƒ…:")
            for pattern in patterns['composite_patterns'][:3]:
                print(f"  - {pattern['pattern_name']} ({pattern['service']})")
                print(f"    ä¸¥é‡ç¨‹åº¦: {pattern['severity']}, ç½®ä¿¡åº¦: {pattern['confidence']:.2f}")
                
                conditions = pattern.get('conditions', {}).get('rules', [])
                if conditions:
                    print(f"    æ¡ä»¶: {len(conditions)} ä¸ªè§„åˆ™")
        
        print(f"\nğŸ’¾ æ¨¡å¼å·²ä¿å­˜åˆ°: {extractor.patterns_file}")
        
    except Exception as e:
        print(f"âŒ æ¨¡å¼æå–å¤±è´¥: {e}")


if __name__ == "__main__":
    main() 